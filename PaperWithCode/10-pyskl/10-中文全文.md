#資料集/ntu60 #資料集/ntu120 #骨架偵測/CTR-GCN #骨架偵測/model/deep-high-resolution-net #動作識別/演算法/STGCN＋＋ #運行框架/pyskl
## 摘要  
我們介紹 PYSKL：一個基於 PyTorch 的開源工具箱，用於基於骨架的動作識別。該工具箱支持多種骨架動作識別算法，包括基於 GCN 和 CNN 的方法。與現有的僅包含一兩個算法的開源骨架動作識別項目相比，PYSKL 在統一框架下實現了六種不同的算法，並採用了最新和經典的優良實踐，旨在簡化效能和效率的比較。我們還提供了一個原創的基於 GCN 的骨架動作識別模型，名為 ST-GCN++，該模型在沒有使用複雜的注意力機制的情況下，達到具有競爭力的識別性能，成為一個強有力的基準模型。同時，PYSKL 支持九個骨架動作識別基準的訓練和測試，並在其中八個基準上達到最先進的識別性能。為促進骨架動作識別的未來研究，我們還提供了大量的訓練模型和詳細的基準結果，以提供一些洞見。PYSKL 已發布於 [https://github.com/kennymckormick/pyskl](https://github.com/kennymckormick/pyskl) 並持續維護。我們將在添加新功能或基準時更新此報告。目前版本對應 PYSKL v0.2。


## **1. 引言**  
基於骨架的動作識別主要是根據人體骨架序列進行動作識別和視頻理解。與其他數據模態（如 RGB / Flow）相比，骨架數據（2D / 3D 人體關節坐標）緊湊但信息豐富，且能抵抗光照變化或場景變化。由於這些優良特性，骨架動作識別在近年來吸引了越來越多的關注。為了實現基於骨架的動作識別，各種算法被相繼開發出來，這些算法主要可以分為基於 GCN 和 CNN 的方法。自從 ST-GCN [23] 首次提出使用圖卷積網絡（GCN）進行骨架數據處理後，基於 GCN 的方法很快成為基於骨架的動作識別的主流範式。ST-GCN 直接將關節坐標序列作為輸入，並通過 GCN 主幹網絡建模骨架數據。該 GCN 主幹網絡由交替的空間圖卷積和時間卷積組成，用於空間和時間的建模。後續的工作[3, 4, 10, 13, 17, 18, 24, 25]繼承了其基本設計，並在以下幾個方面做出了不同的改進：1) 開發更好的圖拓撲結構，無論是手動設置的 [13] 還是可學習的 [3, 17, 24, 25]；2) 與其他輔助任務一起訓練骨架動作識別 [10]；3) 採用了更好的數據預處理、訓練和測試策略 [3, 17, 18]。通過這些改進，識別性能有了顯著的提升。例如，在 NTURGB+D-Xsub [15] 基準上，Top-1 精度提高了超過 10%：從 2018 年的 ST-GCN [23] 的 81.5% 提高到了 2021 年的 CTR-GCN [3] 的 92.4%。

儘管有了顯著的改進，不同 GCN 方法的設置並不完全一致。例如，ST-GCN 只報告了單一關節流的識別性能，而大多數後續工作報告了關節 / 骨流（首次由 2s-AGCN [17] 提出）或甚至四個流（首次由 MS-AAGCN [18] 提出）的集成性能。此外，數據預處理技術（骨架對齊、時間填充、去噪等）也有很大不同。然而，現有的開源庫[3, 13, 18, 23]僅實現了單一算法，且各自採用其獨有的實踐方法。據我們所知，尚未有項目在統一的設定下比較這些架構。因此，我們開發了 PYSKL，它在統一的框架下實現了多個具有代表性的 GCN 方法。我們在多個基準上，基於最新和經典的優良實踐對每個算法進行了訓練和測試。令人驚訝的是，我們發現不同 GCN 方法的識別性能差異並不大：在四個 NTURGB+D 基準上的 Top-1 精度的最大偏差均小於 2%。尤其是在 NTURGB+D XView 上，當前最先進的 CTR-GCN [3] 僅比最早的 ST-GCN [23] 高出 0.5%。

基於初步實驗，我們發現優良實踐對於實現高識別性能的貢獻遠大於複雜的架構設計。本報告呈現了我們在訓練基於 GCN 模型進行骨架動作識別時所採用的所有優良實踐。這些實踐涵蓋了不同方面，包括數據預處理、空間 / 時間數據增強，以及超參數設置。除此之外，我們還提出了一個名為 ST-GCN++ 的原創 GCN 算法。在 ST-GCN 的基礎上進行簡單的修改後，我們在沒有使用複雜的注意力機制的情況下，實現了與當前最先進模型相媲美的識別性能。ST-GCN++ 可以作為未來基於骨架的動作識別研究中的強力基準模型。

另一種基於骨架的動作識別範式是利用卷積神經網絡（CNN）處理骨架數據。這些方法[5,6,22]將人體關節表示為高斯圖並將其聚合為偽圖像或視頻片段。生成的輸入由 2D-CNN 或 3D-CNN 處理。我們在 PYSKL 中實現了一個基於 3D-CNN 的最新方法 PoseC3D [6]。PoseC3D 可以在骨架動作識別基準上實現強大的識別性能，並且相較於 GCN 具有獨特的優勢（如魯棒性、可擴展性、互操作性）。然而，由於其 3D-CNN 主幹，該方法比大多數現有的 GCN 方法要沉重得多。

總結來說，PYSKL 實現了六種具有代表性的基於骨架的動作識別方法，並支持九個不同的基準。它提供了針對五種 GCN 的基於骨架動作識別的廣泛基準結果，包括四個 NTURGB+D 基準、四種骨架模態和兩種註釋類型（3D / 2D 骨架）。

PYSKL 發布於 https://github.com/kennymckormick/pyskl，並採用了 Apache-2.0 許可證。該資源庫包含所有源代碼、大規模模型庫、詳細的安裝說明、數據集準備以及（分佈式）訓練和測試說明。PYSKL 還提供了可視化 2D / 3D 骨架的工具，並可在沒有骨架信息的自定義數據集上執行基於骨架的動作識別。


## 2. 基於 GCN 的方法

### 2.1 基於 GCN 方法的優良實踐

#### 2.1.1 數據預處理  
不同視頻的骨架序列可能具有不同的時間長度、不同的人數，並且可能由來自不同視角或不同設置的傳感器捕獲。對於使用 Kinect 傳感器 [26] 捕獲的 3D 骨架數據 [12, 15]，採用了各種預處理方法。ST-GCN 不使用額外的預處理，將所有序列通過零填充到最大長度。然而，2s-AGCN 執行預規範化，具體做法是：1）將第 1 幀中人的中心點與 3D 笛卡爾坐標系的原點對齊；2）旋轉所有骨架，使得第 1 幀中人的脊柱與 3D 笛卡爾坐標系的 z 軸平行。此外，2s-AGCN 使用循環填充將骨架序列填充到最大長度。CTR-GCN 採用了與 2s-AGCN 相同的空間預處理方式，但保持每個骨架序列的原始長度，並使用不同的標準進行手動去噪。PYSKL 採用了 CTR-GCN 的預處理方法。對於由姿勢估計器 [1,20] 預測的 2D 骨架，我們根據 [23] 將它們預規範化到一個固定範圍（如 [0, 1] 或 [-1, 1]）。我們還進行了簡單的基於姿勢的跟蹤，為 NTURGB+D 數據生成 1 或 2 個骨架序列。

#### 2.1.2 時間增強  
大多數 GCN 研究不使用任何時間增強技術。在具有代表性的 GCN 方法中，CTR-GCN 採用了隨機裁剪作為時間增強技術。它從整個骨架序列中裁剪一個子串（子串長度比例可能從 50% 到 100% 不等），並使用雙線性插值將該子串調整到固定長度 64。受 [6] 啟發，我們使用均勻取樣作為時間增強策略。為了生成長度為 M（PYSKL 中 M=100）的骨架序列，我們將原始序列均勻地分成 M 個長度相等的部分，並在每個部分中隨機取樣一幀。取樣的骨架將重新組合並形成輸入序列。通過均勻取樣，我們可以生成與源數據分佈相似的眾多數據樣本（不使用插值）。

#### 2.1.3 超參數設置  
在使用 GCN 進行骨架動作識別的先前研究中，超參數設置差異很大。在 PYSKL 中，我們使用相同的超參數設置來訓練所有 GCN 模型。我們將初始學習率設置為 0.1，批次大小設置為 128，並使用 CosineAnnealing 學習率調度器訓練每個模型 80 個世代。對於優化器，我們將動量設置為 0.9，權重衰減設置為 5×10−4，並使用 Nesterov 動量。我們發現對於大多數 GCN 網絡，這種新的超參數設置相比於使用 MultiStep 學習率調度器的先前設置，能夠帶來更好的識別性能。

### 2.2 ST-GCN++ 的設計  
我們還提出了一個原創的 GCN 模型，名為 ST-GCN++。僅通過對原始 ST-GCN 進行簡單修改，ST-GCN++ 便實現了與當前最先進的複雜注意力機制方法相媲美的強大識別性能。同時，計算開銷也有所減少。ST-GCN++ 修改了交錯的空間模塊（空間圖卷積）和時間模塊（時間 1D 卷積）的設計。

#### 2.2.1 空間模塊設計  
在 ST-GCN 中，預定義的稀疏係數矩陣用於融合屬於同一個人的不同關節的特徵，這些係數矩陣是基於預定義的關節拓撲結構生成的。與此同時，ST-GCN 還使用一組可學習的權重重新調整係數矩陣中的每個元素。然而，在 ST-GCN++ 中，我們僅使用預定義的關節拓撲結構來初始化係數矩陣，並在訓練過程中使用梯度下降迭代更新係數矩陣，且不施加任何稀疏約束。此外，我們還在空間模塊中添加了一個殘差連接，這進一步提升了空間建模能力。

#### 2.2.2 時間模塊設計  
傳統的 ST-GCN 使用單個核大小為 9 的 1D 卷積來進行時間建模。大核覆蓋了廣泛的時間感受野。然而，這種設計缺乏靈活性，並導致了冗餘的計算和參數。受 [3, 13] 啟發，我們使用多分支時間卷積網絡（TCN）取代單分支設計。所採用的多分支 TCN 包含六個分支：一個 '1x1' 卷積分支、一個最大池化分支，以及四個核大小為 3 的時間 1D 卷積分支，膨脹係數從 1 到 4。不僅如此，它首先使用 '1x1' 卷積轉換特徵，並將其分成六個相同通道寬度的組。然後，每個特徵組通過單個分支處理。六個輸出將被串接在一起，並通過另一個 '1x1' 卷積進行處理，以形成多分支 TCN 的輸出。新的 TCN 設計不僅提升了時間建模能力，還因每個單一分支的通道寬度減少，節省了計算成本和參數。


### 2.3 基準測試 GCN 算法  
在 PYSKL 中，我們對四種具有代表性的 GCN 方法進行了基準測試：ST-GCN [23]、AAGCN [18]、MS-G3D [13]、CTR-GCN [3]，以及原創的 ST-GCN++，這些方法在四個 NTURGB+D 基準 [12, 15] 上進行測試。對於骨架標註，我們考慮了使用 CTR-GCN 預處理生成的 3D 骨架數據，以及由 HRNet [20] 估計的 2D 骨架數據。我們分別報告了關節流、骨骼流、雙流融合（關節 + 骨骼）、以及四流融合（關節 + 骨骼 + 關節運動 + 骨骼運動）的 Top-1 精度。此報告列出了使用 3D 骨架數據的基準結果於表 1、2。使用 2D 骨架數據的結果可參見資源庫。

與最初報告的性能不同，我們發現不同 GCN 方法之間的精度差距要小得多。在所有 NTURGB+D 基準上，Top-1 精度的極端偏差小於 2%。除 CTR-GCN 外，其他所有算法的再現結果均優於先前報告，這得益於採用了優良實踐。此外，我們的 ST-GCN++ 在識別性能上與最先進的 GCN 方法相媲美，並具有更簡單的設計、更少的參數和更少的浮點運算次數（FLOPs）。

![[PaperWithCode/10-pyskl/Table1.png]]
表 1. 在 NTURGB+D 基準上對基於 GCN 骨架的動作辨識演算法進行基準測試。輸入是具有 25 個關節的 3D 骨架。我們將輸入長度設為 100，輸入人數為 2，並應用第 2.1 節中介紹的所有良好實踐。

![[PaperWithCode/10-pyskl/Table2.png]]
表 2. 在 NTURGB+D 120 基準上對基於 GCN 骨架的動作辨識演算法進行基準測試。輸入是具有 25 個關節的 3D 骨架。我們將輸入長度設為 100，輸入人數為 2，並應用第 2.1 節中介紹的所有良好實踐。

### 2.4 空間增強  
我們還在骨架動作識別中採用了空間增強技術。我們在 PYSKL 中實現了三種增強：1) 隨機旋轉：使用相同的隨機角度 θ（θ = (θx, θy) ∈ R2 或 θ = (θx, θy, θz) ∈ R3）旋轉所有骨架，每個 θ 中的元素從均勻分佈 [-0.3, 0.3] 中取樣。2) 隨機縮放：使用相同的縮放係數 r（r = (rx, ry) ∈ R2 或 r = (rx, ry, rz) ∈ R3）縮放骨架序列中的所有關節坐標，r 中的每個元素從均勻分佈 [-0.1, 0.1] 或 [-0.2, 0.2]（對於 3D / 2D 骨架）中取樣。3) 隨機高斯噪聲：隨機為每個關節添加一個小的高斯噪聲，噪聲可以是特定於某幀的或幀無關的。我們進行了大量實驗來驗證這三種空間增強的有效性（結果見表 3）。我們發現，在這三種增強中，隨機旋轉對 3D 骨架有效；隨機縮放對 2D 和 3D 骨架都有效；而隨機高斯噪聲對任何類型的骨架都無效。我們使用隨機旋轉和隨機縮放對 3D 骨架進行訓練，共 120 個世代。表 4 顯示，在四個 NTURGB+D 基準中的三個上，ST-GCN++ 超越了當前最先進的 CTR-GCN。

![[PaperWithCode/10-pyskl/Table3.png]]
表 3. 在兩個 NTURGB+D 120 基準上使用 STGCN++ 對空間增強進行基準測試。隨機旋轉適用於 3D 骨架，而隨機縮放適用於 2D 和 3D 骨架。

![[PaperWithCode/10-pyskl/Table4.png]]
表 4. 經過良好實踐和空間增強訓練的 ST-GCN++ 在 4 個 NTURGB+D 基準測試中的 3 個上超過了 CTR-GCN（官方性能）。
## 3. 基於 CNN 的方法  
PYSKL 也實現了基於 3D-CNN 的方法 PoseC3D [6]。PoseC3D 將 2D 人體骨架作為輸入，首先根據 2D 關節坐標生成高斯圖，然後將它們組織為 3D 熱圖體積。PoseC3D 可以使用任意 3D-CNN 來處理 3D 熱圖體積。在 PYSKL 中，我們支持三個主幹網絡：C3D [21]、SlowOnly [8]、X3D [7]，並發布了在七個不同數據集上訓練的 PoseC3D 模型：NTURGB+D [15]、NTURGB+D 120 [12]、Kinetics-400 [2]、UCF101 [19]、HMDB51 [9]、FineGYM [16] 和 Diving48 [11]。PoseC3D 具有良好的時空建模能力，並在 9 個基準中的 6 個上實現了最先進的識別性能。然而，使用 3D-CNN 處理骨架數據耗費更多計算資源，速度也比大多數具有代表性的 GCN 方法慢得多。

## 4. 結論  
我們已經公開發布了 PYSKL，它在骨架動作識別方面有廣泛的基準測試。PYSKL 在統一框架下實現了六種具有代表性的算法，並在九個不同的骨架動作識別基準上進行了訓練，並在其中八個基準上實現了最先進的識別性能（見表 5）。它提供了訓練骨架動作識別模型的優良實踐，並提供了廣泛的基準測試。此外，它還引入了一個簡單且強大的基準模型 ST-GCN++，在 NTURGB+D 基準上超越了之前的最先進模型。我們希望這個資源庫，以及所有公開的訓練配置和模型權重，能促進該領域未來的研究。

![[PaperWithCode/10-pyskl/Table5.png]]
表 5. PYSKL 在九個基準測試中所獲得的最佳效能。在 9 個骨架動作辨識基準中的 8 個中，PYSKL 實現了最佳辨識精度。我們報告除 FineGYM 之外的所有基準的 Top-1 準確度（我們報告平均類別準確度）。對於 Diving48，我們使用 V2 註解。