#資料集/ntu60 #資料集/ntu120 #資料集/PKUMMD #骨架偵測/tools/Kinectv2  #動作識別/演算法/STGCN #動作識別/演算法/BERT  #運行框架/MS2L
## **摘要**

零樣本基於骨架的動作識別旨在在訓練過程中==使用已知類別的數據來識別未知類別的動作。關鍵在於建立從已知類別到未知類別之間視覺空間與語義空間的連接。==之前的研究主要集中於將序列編碼為單一特徵向量，隨後將特徵映射到嵌入空間中的相同錨點。然而，它們的性能受以下兩個因素的限制：1）忽略了全局視覺/語義分佈的對齊，導致無法捕捉這兩個空間之間的真正相互依賴性；2）忽略了時間信息，因為具有豐富動作線索的逐幀特徵直接被匯集為單個特徵向量。我們提出了一種新的零樣本骨架動作識別方法，通過互信息（MI）的估計和最大化來解決這些問題。具體而言：1）我們通過最大化視覺空間和語義空間之間的互信息來進行分佈對齊；2）我們利用時間信息來估計互信息，通過隨著觀察到更多的幀來鼓勵互信息的增加。對三個大規模骨架動作數據集進行的廣泛實驗證實了我們方法的有效性。

## **1. 引言**

人體動作識別在許多現實應用中成為了不可或缺的組成部分，包括但不限於安全領域和人機互動。由於姿勢估計技術 [13] 和傳感器技術 [45] 的發展，獲取骨架數據變得相對簡單。因此，與傳統的RGB視頻數據相比，骨架數據憑藉其對外觀和背景變化的魯棒性以及能夠提供無偏的個體表示，成為了一種有前途的替代方案。

許多研究者探索了針對該任務的完全監督方法，但這些方法需要大量的標註訓練樣本。然而，處理現實場景中的大量動作類別並不經濟，因為許多動作的樣本收集既耗時又昂貴。因此，零樣本學習 [18, 29] 用於識別那些沒有訓練樣本但只有某些語義信息（如類別名稱、屬性或描述）的新類別。由於包含深度信息及動作語義的複雜性，為3D骨架動作數據進行註釋和標註的難度進一步增加。因此，在實際應用中，零樣本骨架動作識別具有高度的應用價值，因為它顯著減少了收集和標註新動作的需求。

如圖1所示，在給定預提取的視覺和語義特徵後，零樣本學習的核心在於為已知類別中的視覺空間和語義空間建立一個連接模型。在測試階段，學到的模型用於促進從已知類別到未知類別的知識轉移。為了解決這個遷移學習問題，零樣本動作識別依賴於外部知識庫，即從預訓練的大規模語言模型（如Sentence-Bert [28] 或CLIP [27] ）中獲取的每個類別標籤的語義嵌入。如何有效利用語義信息在兩種不同模態之間架起橋樑，對縮小這一間隙至關重要。

![[PaperWithCode/18-SMIE/Figure1.png]]
圖1：零樣本學習的核心在於在訓練階段建構視覺特徵v和語意特徵a之間的連結模型T。在測試時，學習的模型 T 用於預測給定的未見類視覺特徵的最相容的語義屬性。

已有一些研究針對零樣本骨架動作識別進行探索。現有方法 [9, 14] 將動作序列嵌入視覺特徵中。為了在訓練階段的已知類別中建立視覺和語義空間的連接模型，通過學習兼容的投影函數 [14] 或深度度量 [6] 來實現。在測試階段，通過在投影的共同空間中或者通過學到的度量，測量測試動作序列的視覺特徵與未知類別的句子嵌入 [14] 或詞性標籤詞 [41] 之間的相似性。然而，這種投影操作僅僅是將視覺或語義特徵映射到嵌入空間中的某個共同錨點，忽略了視覺和語義特徵分佈之間的全局對齊。此外，學到的投影或度量無法充分利用語義信息來捕捉這兩種模態之間的關聯。它們試圖在未對齊分佈的情況下進行跨模態重建，這很難實現，因為視覺空間和語義空間之間存在顯著的差異，最終導致在具有多樣分佈的新類別上難以進行泛化。

其次，在零樣本動作識別任務場景中，由於某些語義類別需要動態信息來區分彼此，信息丟失問題變得尤為嚴重。例如，“走路”和“跳躍”僅在局部區域存在差異，因為這兩個動作序列的初始幀相似，只有在觀察到人類上升的過程後才能識別出“跳躍”。因此，對於人體動作，利用固有的時間動態信息也有助於零樣本連接模型的泛化能力。

本文提出了一種基於骨架的互信息估計和最大化框架，用於零樣本動作識別（SMIE）。為了更好地捕捉視覺空間和語義空間之間的依賴關係，我們的方法避免直接映射，而是通過一個全局對齊模塊來對齊這兩個空間的分佈。該模塊利用互信息作為相似性度量，並基於Jensen-Shannon散度（JSD）來估計互信息，最大化成對的視覺和語義特徵之間的互信息，同時最小化未配對的視覺和語義特徵之間的互信息。神經網絡被用作連接模型來估計JSD估計器中的相似性分數，該分數在測試階段用於未知類別。隨後，考慮到動作的固有時間信息，SMIE提出了一個時間約束模塊，鼓勵隨著更多動作部分的執行，視覺和語義特徵之間的互信息增加。具體來說，JSD估計器應用對比學習來估計全局互信息。成對的視覺和語義特徵構成正樣本，而未配對的特徵構成負樣本。為了感知動作序列中包含更多區別性信息的關鍵幀，時間約束模塊計算每個序列的運動注意力，並屏蔽具有較高注意力的關鍵幀以生成額外的正樣本，這些樣本包含部分時間信息丟失。在訓練過程中，與全局互信息保持一致，計算具有相同負樣本的時間約束互信息並使其小於全局互信息。

本文的主要貢獻如下：
- 我們提出了一種基於骨架的互信息估計和最大化框架（SMIE），這是一種新的零樣本骨架動作識別方法，通過最大化互信息來捕捉視覺空間和文本語義空間分佈之間的複雜統計關聯。
- 提出了一個新穎的時間約束模塊來計算時間約束互信息，並應用時間排序損失來幫助連接模型捕捉動作的固有時間信息。
- 通過廣泛的實驗和分析，證明了所提方法的有效性，其性能相比基線方法有了大幅度提升。


## **2. 相關研究**

**零樣本動作識別**  
大多數現有的零樣本視頻分類方法旨在通過特徵投影在視覺和語義空間之間建立聯繫，這些方法主要關注視覺空間 [10, 17, 40] 、語義空間 [1, 47] 以及中間空間 [7, 42] 。具體而言，視覺特徵首先使用預訓練的網絡（如3D卷積網絡（C3D） [36] 、ResNet [11] 、Inflated 3D Network（I3D） [3] ）從視頻中提取出來，然後將視覺或語義特徵映射到嵌入空間中的固定錨點。與這些工作不同，我們專注於零樣本基於骨架的動作識別，骨架動作序列的視覺特徵與RGB視頻有很大不同。我們使用互信息來關聯骨架的視覺和語義特徵，而不是傳統的投影方法。

**基於骨架的動作識別**  
隨著Kinect攝像頭等高精度深度傳感器以及姿勢估計算法 [2, 32] 的發展，基於骨架的動作識別最近受到越來越多的關注。基於人體骨架的表示 [8, 19] 對外觀和背景環境的變化具有很強的魯棒性，每個骨架包含不同類型的關節，每個關節記錄其3D位置。具體來說，骨架通常被組織為偽圖像 [15, 16] 或長期上下文信息的序列 [5] ，並將其輸入CNN/RNN進行處理。後來，ST-GCN [43] 根據人體關節的自然連接構建了一個預定義的空間圖，並利用圖卷積網絡（GCN）整合骨架關節信息。針對連續幀，ST-GCN在對應的關節之間構建了時間邊。之後，提出了許多ST-GCN的變體 [4, 31, 39, 44] ，這些變體包含更多的數據流或添加了注意力機制。在本文中，我們採用ST-GCN [43] 和Shift-GCN [4] 作為骨架動作的視覺特徵提取主幹網絡。為了探索骨架數據的時間關係，我們保留了預訓練的GCN模型，並通過輸入不同索引的骨架幀來獲取部分視覺特徵。

**零樣本基於骨架的動作識別**  
儘管這一領域具有重要性，但針對零樣本骨架動作識別的研究較少。DeViSE [6] 和RelationNet [14] 被擴展來解決這一問題，這些方法通過ST-GCN從骨架序列中提取視覺特徵，並通過Word2Vec [23] 或Sentence-Bert [28] 提取語義嵌入。DeViSE使用了一個簡單的可學習線性投影來在視覺和語義特徵空間之間建立聯繫。在此基礎上，RelationNet使用屬性網絡和關係網絡來實現相同的目標。在上述工作中，投影操作需要在視覺-語義嵌入中構建投影，他們將視覺或語義特徵映射到嵌入空間中的固定錨點，而不考慮語義特徵的全局分佈。最近，SynSE [9] 使用一個生成多模態對齊模塊將視覺特徵與詞性標註的詞語對齊，該方法有效，但需要額外的詞性語法信息來將標籤劃分為動詞和名詞。我們的方法與這些方法的不同之處在於兩個方面：第一，我們最大化了兩個模態之間的互信息，使視覺和語義特徵的分佈可以對齊，並且可以利用語義分佈的全局信息來幫助知識從已知域轉移到未知域。第二，我們使用了一個時間約束模塊，鼓勵隨著觀察到的幀數增加，視覺和語義特徵之間的互信息也增加，從而充分利用骨架序列的時間信息。

**零樣本學習中的互信息**  
最近，視覺空間和語義空間之間的互信息在零樣本學習中得到了探索。在 [33] 中，從其他具有相同標籤的圖像中提取局部片段，以估計局部的互信息並實現解釋性。在 [34] 中，互信息被用來學習潛在的視覺和語義表示，以便多模態能夠對齊，從而實現廣義的零樣本學習。與上述方法不同，我們使用互信息來橋接基於骨架的視覺空間和文本標籤語義空間，並對互信息施加了新穎的約束，以捕捉骨架序列的時間語義。

## **3. 方法**

### **3.1 問題定義**  
本文探討的零樣本學習設定與 [14] 中的相同，即模型在已知類別的數據上進行訓練，並在與訓練類別不重疊的未知類別上進行測試。具體來說，訓練數據集由骨架序列及其對應的已知類別名稱組成。每個訓練樣本記為 $(x^s, e^s)$，其中 $(x^{S},e^{S}),x^{S}\in\mathbb{R}^{K\times J\times C}$ 表示具有 $K$ 幀、每幀包含 $J$ 個記錄的 3D 關節的訓練骨架序列，而 $e^s$ 則是對應的類別名稱。測試數據來自未知類別，測試數據集的一個樣本記為 $(x^u, e^u)$。

通常，我們使用一個已在已知類別上預訓練過的視覺特徵提取器 $F_v$ 和一個已在大規模語言模型上預訓練過的語義特徵提取器 $F_e$，這兩個提取器分別輸入骨架序列和類別名稱來獲取視覺特徵 $v$ 和語義特徵 $a$。然後，我們使用一個沒有可學參數的層歸一化層 (Layer-Norm) $N$ 來對視覺特徵進行歸一化：
$$
v^i = N(F_v(x^i)), a^i = Fe_(e^i), i \in \{s, u\} \tag{1}
$$
令 $V$ 和 $A$ 分別表示視覺和語義特徵的隨機變量。

零樣本學習的目標是通過基於已知類別訓練數據學習的模型來分類未知類別的樣本。語義特徵的利用非常重要，因為語義空間在已知和未知類別之間是共享的，這有助於學習的模型將知識轉移到不同的領域。為簡便起見，在接下來的模型訓練介紹中，我們省略已知 (s) 和未知 (u) 類別的下標。

### **3.2 方法概述**  
如圖2所示，我們提出了一個基於骨架的互信息估計和最大化框架 (SMIE)。我們的SMIE由兩個模塊組成：==全局對齊模塊使用互信息估計和最大化來捕捉視覺和語義分佈之間的統計關係。時間約束模塊則用於使連接模型 T 能夠感知序列中的關鍵幀，從而探索動作的動態並捕捉內在的時間信息。==

在推理階段，經過訓練的互信息估計網絡 $T$ 計算測試視覺序列與所有未知類別的語義特徵之間的相似度分數 $g$，並選擇相似度分數最高的未知類別作為預測結果。

![[PaperWithCode/18-SMIE/Figure2.png]]
圖 2：我們提出的 SMIE 框架包括一個用於估計視覺和語義特徵之間的互資訊的全局對齊模組，以及一個用於將時間動態資訊納入估計網路的時間約束模組。

### **3.3 全局對齊**  
先前的零樣本基於骨架的動作識別方法 [6, 14] 學習了將視覺特徵與對應語義特徵在已知類別中拉近的投影，但未考慮特徵的整體分佈。由於視覺和語義空間之間的巨大差距，很難跨越這一跨域鴻溝並將投影泛化到未知類別。

為了解決這個問題，我們設計了一個全局對齊模塊，通過最大化視覺和語義特徵隨機變量 V 和 A 之間的互信息 $I(V; A)$ [25, 38] 來學習一個估計網絡 $T$：
$$
I(V; A) = D_{KL}(p(v, a) || p(x)p(a)) = E_{p(v,a)}\left[\log \frac{p(v | a)}{p(v)}\right]. \tag{2}
$$
其中，$D_{KL}$ 是 $p(v, a)$ 和 $p(v)p(a)$ 之間的 KL散度 (KL-divergence)，表示視覺特徵 $v$ 和語義特徵 $a$ 的聯合分佈與邊緣分佈的乘積之間的差異。聯合分佈的信息可以被利用，幫助模型使用全局語義屬性。同時，互信息可以用視覺和語義隨機變量的聯合熵和條件熵表示：
$$
I(V; A) = H(V, A) - (H(V | A) + H(A | V)). \tag{3}
$$
如上所述，最大化 $V$ 和 $A$ 之間的互信息等同於最大化它們之間的共同信息，即聯合熵與條件熵之間的差異。

根據公式(2)，我們不直接建模 $p(v | a)$，而是利用互信息將 $V$ 和 $A$ 編碼為緊湊的分佈向量表示，通過從數據中學習的連接網絡來實現。這樣，視覺和語義特徵之間捕捉到的共享信息可以保持更好的全局結構，低層次信息和噪音將被捨棄。

然而，直接計算高維空間中兩個隨機變量的互信息極其困難。受Jensen-Shannon散度（JSD） [24] 啟發，我們提出了一個學習估計網絡 $T$，它將全局視覺特徵 $v$ 和語義特徵 $a$ 作為輸入。網絡的輸出可以作為視覺和語義特徵之間的相似度分數 $g$，並在測試階段用來將骨架序列與未知類別匹配。

估計網絡 $T$ 可以通過最大化以下JSD估計器來進行訓練：
$$\begin{aligned}I(V;A)\approx m&=\mathbb{E}_{p(v,a)}[-f_{sp}(-T(v,a))]\\&-\mathbb{E}_{p(v)p(a)}[f_{sp}(T(v^{\prime},a))],\end{aligned} \tag{4}$$
其中 m 是估計的互信息，如圖2所示。需要注意的是，$(v, a)$ 是配對的視覺/語義特徵，而 $(v^{\prime}, a)$ 是負樣本對。具體來說，$v$ 是從與 $a$ 相關的骨架序列 $x$ 提取的視覺特徵，$v^{\prime}$ 則是來自其他類別的負樣本 $x^{\prime}$ 提取的視覺特徵。fsp 是 soft-plus 函數，定義為 $f_{sp}(z) = log(1 + e^z)$。然後，將視覺特徵 $v$ 與語義特徵 $a$ 拼接形成正樣本對，而負樣本對則由該語義特徵與另一序列的視覺特徵 $v^{\prime}$  拼接形成。這兩個樣本對將被送入估計網絡 T，通過對比學習獲得分數 $g$ 和 $g^{\prime}$ 。

通過這種方式，估計網絡促使語義特徵與對應的視覺特徵的相似度大於那些未配對的視覺特徵。為了最大化全局對齊的估計互信息 $m$，我們得到了以下損失函數：
$$\mathcal{L}_{1}=-m. \tag{5}$$
在訓練過程中，通過梯度下降更新估計網絡 T 的參數。


### **3.4 時間約束模組**
==與影像數據相比，3D 人體骨架數據更為複雜，因為它包含了額外的時間維度。充分利用骨架序列中的時間動態信息可以幫助模型捕捉不同類別之間的細微差異。為了結合這些時間信息，我們提出了一個時間約束模組。==

通常，對於人體動作序列，觀察的幀數越多，模型能捕捉到的動態信息就越多，這促使視覺特徵與其對應的語義特徵之間的關聯性更強。此外，動作序列中的關鍵幀往往包含更豐富的區分信息，因此這些幀的序列損失與其語義標籤的相關性較低。受 PSTL [46] 的啟發，我們利用骨架數據的運動來找到每個序列的關鍵幀。進一步地，我們採用了雙向運動注意力來增強效果。如圖2下半部分所示，為了獲取注意力掩蔽樣本，我們首先計算每個動作序列的雙向動作注意力。具體來說，該序列的運動 $p\in\mathbb{R}^{K\times J\times C}$ 是通過幀與幀之間的時間位移計算得出的： $p^{\text{nex}}_{k,j,c} = x_{k+1,j,c} - x_{k,j,c},$
這表示每一幀中動作的後續變化。接下來，我們將當前幀與其前一幀之間的位移也納入運動信息中：$p^{\text{pre}}_{k,j,c} = x_{k-1,j,c} - x_{k,j,c}.$該序列的雙向運動可定義為：
$$
p_{k,j,c} = (p^{\text{nex}}_{k,j,c})^2 + (p^{\text{pre}}_{k,j,c})^2. \tag{6}
$$
然後，我們計算每一幀的平均運動值來獲取 $p_k$：
$$
p_k = \frac{1}{J × C} \sum_{j=1}^{J} \sum_{c=1}^{C} p_{k,j,c}. \tag{7}
$$
根據雙向運動 $p_k$，我們可以獲得一個幀的總運動率，該值作為雙向注意力權重：
$$
q_k = \frac{p_k}{\sum_{i=1}^{K} p_i}. \tag{8}
$$
接著，我們選擇具有最高注意力分數的前 $P$ 幀，即 $q_{k1}, ..., q_{kP}$，這些幀作為關鍵幀，包含關於動作的更多區分信息。我們將這些關鍵幀從原始骨架序列中掩蔽以構建注意力掩蔽的樣本序列 $\hat{x}$，該序列具有部分信息丟失，並且其語義特徵的相關性較低。藉助視覺特徵提取器 F 和層歸一化層 N，提取的時間約束視覺特徵 $\hat{v}$ 與對應的語義特徵 $a$ 拼接，形成時間約束的正樣本對。

與全局對齊模組中的 JSD 估計器類似，時間約束互信息 $\hat{m}$ 被定義如下：
$$\hat{m}=\mathbb{E}_{p(\hat{v},a)}[-f_{sp}(-T(\hat{v},a))]\\-\mathbb{E}_{p(\hat{v})p(a)}[f_{sp}(T(v^{\prime},a))], \tag{9}$$
此處，時間約束視覺特徵與對應語義特徵之間的互信息被最大化。需要注意的是，負樣本仍由原始的負樣本 $x'$ 和未配對的語義特徵 $a$ 組成，以確保負樣本空間的一致性。我們的時間約束模組旨在鼓勵連接模組在互信息估計過程中感知關鍵幀的重要性。因此，我們使用鉸鏈損失來強制全局互信息 $m$ 大於部分互信息 $\hat{m}$：
$$
\mathcal{L}_2 = \max(0, \beta - (m - \hat{m})), \tag{10}
$$
其中，$\beta$ 是控制兩種互信息之間距離的超參數。通過調整 $\beta$，模型可以適應不同的數據集。簡而言之，時間約束模組作為 JSD 估計器的一種正則化，有助於模型結合動態信息並變得更加健壯。

最終的損失函數將全局互信息最大化項與時間約束項結合在一起，如下所示：
$$\mathcal{L}=\mathcal{L}_{1}+\lambda\mathcal{L}_{2}. \tag{11}$$
其中 $\lambda$ 是權衡參數，在所有實驗中設定為 0.5。

## **4 實驗**

### **4.1 資料集**

NTU-RGB+D 60 [30] 包含 56,578 筆骨架序列，涵蓋 60 個動作類別，由 40 名志願者執行。骨架序列由 Microsoft Kinect 感測器收集，每個受試者包含 25 個關節。該資料集有兩種官方的資料集劃分方式：
1) **Cross-Subject (xsub)**：訓練集包含一半的受試者，其餘的受試者組成測試集；
2) **Cross-View (xview)**：不同視角的數據組成訓練集和測試集。

NTU-RGB+D 120 [21] 是 NTU-60 的擴展版本，由 106 名志願者執行，共包含 113,945 筆骨架序列，涵蓋 120 個動作類別。NTU-120 也有兩種官方劃分方式：
1) **Cross-Subject (xsub)**：53 名受試者屬於訓練集，其餘受試者組成測試集；
2) **Cross-Setup (xset)**：訓練集由偶數編號相機拍攝的數據組成，測試集由奇數編號相機拍攝的數據組成。

PKU-MMD [22] 包含近 20,000 個動作樣本，涵蓋 51 個類別，由 66 名受試者執行，數據由 Kinect v2 感測器從多個視角拍攝。該資料集分為兩部分：
1) **Part I** 包含 21,539 個樣本；
2) **Part II** 包含 6,904 個樣本。

### **4.2 實驗細節與基準線**

**SMIE 的詳細實現**：我們遵循 Cross-CLR [19] 中的數據處理流程，去除了無效幀，並通過線性插值將骨架序列重新調整為 50 幀。ST-GCN [43] （具有 16 個隱藏通道）被用作視覺特徵提取器，提取的特徵維度為 256。對於語義特徵，我們使用 Sentence-Bert [28] 來獲取 768 維的詞嵌入，並對所有語義特徵進行 L2 正則化，以提高訓練階段的穩定性。在所有實驗中，我們使用 Adam 優化器和 CosineAnnealing 調度器，訓練 100 個世代，迷你批次大小為 128。學習率對於 NTU-60 和 PKU-MMD 資料集設為 $1e−5$，對於規模較大的 NTU-120 資料集設為 $1e−4$。表1 列出了所有資料集的超參數選擇。P 代表被遮蔽的關鍵幀數量，對於所有三個資料集保持 15。超參數邊界 $β$ 控制全局與時間約束互信息之間的距離。隨著邊界的縮小，時間約束對總損失的影響會增加。表 1 顯示了資料集大小與邊界參數 $β$ 之間的正相關性。較小的資料集需要較小的 $β$ 才能獲得最佳性能。其原因是時間約束模組作為 JSD 估計器的正則化，透過對模型施加必要的限制，防止其過度擬合於有限的資料集。

![[PaperWithCode/18-SMIE/Table1.png]]
表 1：NTU-60、NTU-120 和 PKU-MMD 資料集上的超參數。

**SMIE 模型細節**：在公式 (4) 中的網絡 $T$ 由三層 MLP 構成，並帶有 ReLU 激活函數。對於負樣本，我們在批次中移動骨架視覺樣本，使視覺和語義特徵不對應。

**基準方法**：骨架基於零樣本學習的核心在於評估連接模型的有效性，該模型作為視覺和語義空間之間的中介。最近的方法 SynSE [9] 遵循這一主要思想，並提供了多種零樣本學習的比較方法，例如 Devise [6] 、ReViSE [37] 、RelationNet [14] 、JPoSE [41] 和 CADA-VAE [29] 。具體來說，DeViSE 和 RelationNet 都使用線性投影將視覺特徵和語義特徵映射到相同的空間。在投影後，DeViSE 計算投影後視覺和語義特徵之間的點積相似度，而 RelationNet 則利用關係模組來獲取投影特徵之間的相似度。ReViSE 使用最大均值差異損失作為跨域學習標準來對齊潛在嵌入。JPoSE 在文本和骨架數據之間執行跨模態細粒度動作檢索。它學習 PoS 感知的嵌入，並為每個 PoS 標籤構建一個獨立的多模態空間。CADA-VAE 通過對齊的變分自編碼器來學習視覺特徵和語義嵌入的潛在空間。基於上述方法，最新的 SynSE 方法將潛在的骨架視覺表示與 PoS 語法信息融合。我們對 SMIE 與這些基準方法進行了 apple-to-apple 的比較。

### **4.3 與最新技術的比較**

**評估設置**：在零樣本學習中，不同類別劃分的選擇會產生不同的已知類別和未知類別集合，這對實驗結果有顯著影響。此外，特徵提取器的選擇也會影響準確性。為了與最新的 SynSE 方法進行直接比較並充分展示我們連接模型的有效性，我們採用了與 SynSE 相同的實驗設置，這意味著我們使用相同的類別劃分，並採用了其代碼庫中提供的預提取的視覺和語義特徵。具體來說，SynSE 在 NTU-60 和 NTU-120 資料集中提供了兩種類別劃分。在 NTU-60 資料集中，SynSE 提供了 55/5 和 48/12 的劃分，分別包含 5 個和 12 個未知類別。而對於擁有更多類別的 NTU-120 資料集，SynSE 提供了 110/10 和 96/24 的劃分。本研究中使用的視覺特徵提取器是 Shift-GCN [4] ，而語義特徵提取器是 Sentence-Bert [28] 。

**結果與分析**：表2展示了我們與基準方法在 NTU-60 和 NTU-120 資料集上的比較結果。對於 NTU-60 資料集的 55/5 和 48/12 分割，我們的 SMIE 分別比 SynSE 提高了 2.17% 和 6.88%（相對提高 2.86% 和 20.66%）。對於 NTU-120 資料集的 110/10 和 96/24 分割，SMIE 分別達到 3.05% 和 6.60%（相對提高 4.87% 和 17.05%）。隨著未知類別數量的增加，從已知類別到未知類別轉移學習的難度也會增加。值得注意的是，我們提出的 SMIE 方法利用互信息來捕捉全局語義信息，有效地彌合了視覺和語義空間之間的鴻溝，展示了在提升未知類別性能方面的潛力。

![[PaperWithCode/18-SMIE/Table2.png]]
表 2：SMIE 與 NTU-60 和 NTU-120 資料集上最先進的方法的比較。

### **4.4 消融研究**

**優化的實驗設置**：從 SynSE 的實驗設置中，我們發現零樣本學習實驗的目標是驗證學習到的連接模型的有效性。然而，由於不同類別劃分對結果有顯著影響，即使未知類別數量相同，準確性也可能存在很大的偏差。同時，應該盡量減少複雜結構的特徵提取器對結果的影響，以專注於連接模型本身的有效性。因此，我們為零樣本骨架動作識別提供了一個優化的實驗設置。首先，我們將資料集從兩個擴展到三個大型骨架資料集，即 NTU-60、NTU-120 和 PKU-MMD 資料集，這增加了結果的可信度。其次，對每個資料集進行三折測試以消除變異性。每折包含不同的已知和未知類別組合，並報告平均結果。最後，我們遵循大多數基於骨架的自監督方法 [19, 20, 35, 46] ，並應用經典的 ST-GCN [43] 作為視覺特徵提取器，以最小化特徵提取器的影響。本研究中使用的語義特徵提取器與 SynSE 一致，都是 Sentence-Bert。

**優化實驗設置的綜合分析**：在優化的實驗設置下，我們對 SMIE 的時間約束模組進行了消融研究。為了為後續的研究工作提供更多基準結果，我們在此設置下重現了兩個映射方法（DeViSE 和 RelationNet）以及一個分佈對齊方法（ReViSE）。具體來說，對於 NTU-60 和 PKU-MMD 資料集，隨機選擇 5 個未知類別，剩餘的作為已知類別。視覺提取器僅在已知類別上進行預訓練。對於 NTU-120 資料集，未知類別的數量為 10。我們為所有資料集進行了 3 組隨機劃分，並在表 3 中報告了平均結果。我們發現，基於投影的方法取得了相對較低的準確度。透過對兩個領域中的潛在嵌入進行對齊，ReViSE 在三個資料集上取得了一些改進，這進一步證明了全局語義信息對語義特徵的重要性。然而，ReViSE 由於使用了額外的自解碼器，過於複雜。SMIE 在所有資料集上都取得了顯著的改進。具體而言，SMIE 在三個資料集上分別比其他投影方法高出約 13.77%、11.78% 和 15.18%。對於 ReViSE，我們的 SMIE 仍然分別取得了 6.60%、7.05% 和 1.50% 的增幅。透過將互信息作為相似性度量，SMIE 對齊了兩個模態的分佈，並在公式 (3) 中納入了不同特徵之間的更多判別信息。對於時間約束模組的消融研究，表中的 "SMIE (w/o L2)" 表示僅包含全局對齊模組的模型，"SMIE" 表示完整模型。結果顯示，時間約束模組分別在三個資料集上帶來了約 1.40%、1.03% 和 1.01% 的性能提升。結果表明，時間約束有助於整合有用的時間信息。

![[PaperWithCode/18-SMIE/Table3.png]]
表 3：在 NTU-60、NTU-120 和 PKU-MMD 資料集上優化實驗設定下的消融研究。

**超參數的影響**：為了確定 SMIE 中最佳的邊界 β 選擇，我們根據完整模型對其進行了不同設置，並在 NTU-60、NTU-120 和 PKU-MMD 資料集上進行了測試。表 4 顯示了整體結果。隨著邊界的減小，時間約束對總損失的影響增加。我們發現，隨著 $β$ 的增加，性能先提升後下降。在 NTU-60、NTU-120 和 PKU-MMD 資料集中，$β$ 值分別為 0.1、0.5 和 0.01 時取得了最佳結果。邊界的選擇在全局對齊模組和時間約束模組之間保持平衡。當 $β = 1$ 時，時間約束無法得到充分利用，導致性能下降。

![[PaperWithCode/18-SMIE/Table4.png]]
表 4：NTU60、NTU-120 和 PKU-MMD 資料集上 SMIE 中不同邊距 β 的比較。

**不同語義特徵的消融研究**：在優化的實驗設置下，我們還探索了不同語義特徵提取器對實驗結果的影響。如表 5 所示，我們使用 CLIP [27] 替代 Sentence-BERT [28] 作為語義特徵。同樣，使用不同的語義特徵，基於全局對齊的方法仍然比直接映射方法取得了更好的性能。我們提出的 SMIE 在 NTU-60 和 PKU-MMD 資料集上也遠超基準方法。在擁有更多數據的 NTU-120 資料集上，我們的 SMIE 方法與利用額外視覺和文本自解碼器的更複雜的 ReViSE 相當。這些結果證明，無論使用哪種語義特徵提取器，SMIE 都能以簡單高效的結構取得良好的性能。

![[PaperWithCode/18-SMIE/Table5.png]]
表 5：CLIP 語意特徵提取器在 NTU60、NTU-120 和 PKU-MMD 資料集上的結果。

**使用 ChatGPT 擴展類別描述**：傳統方法通常依賴於將類別標籤作為輸入語義特徵提取器，以獲取相應的語義特徵。然而，這些標籤僅包含幾個詞，無法完全準確地描述對應的動作語義。基於此，我們使用 ChatGPT 擴展每個動作標籤名稱為完整的動作描述，然後提取其語義特徵。例如，"穿外套" 可以擴展為 "穿上設計用於覆蓋上半身和手臂的衣物的行為"。按照我們的優化實驗設置，結果如表 6 所示。SMIE_Chat 的顯著改進表明，更全面的動作語義描述能提高語義特徵的表示能力，有助於連接模型捕捉視覺與語義空間之間的關係。

![[PaperWithCode/18-SMIE/Table6.png]]
表 6：ChatGPT 在 NTU-60、NTU-120 和 PKU-MMD 資料集上擴展類別描述的結果。

**定性結果與分析**：我們在圖 4 中可視化了我們方法對四個未知類別的部分預測分數。對於圖 4 (a, b, c) 所示的單人情境來說，"拍手" 和 "咳嗽" 這兩個動作彼此非常相似。我們的模型不僅做出了正確的預測，還為相似類別給出了合理的預測。例如，當真實標籤是 "拍手" 時，預測得分第二高的類別是 "咳嗽"。在圖 4 (d, e) 所示的兩人情境中也得出了類似的結論，證明了我們方法的合理性。

**混淆矩陣的可視化**：圖 3 顯示了 NTU-120 資料集中 3 組隨機類別劃分的混淆矩陣。每個矩陣包含 10 個未知類別，矩陣中的數字代表已分類樣本數量。通過閱讀混淆矩陣，可以了解每個類別的分類準確率和錯誤分類情況。我們觀察到 1) 短動作（如 "跳起" 或 "打哈欠"）容易被錯誤分類，且時間約束對它們的效果較弱；2) 某些動作如 "撿起" 和 ”撕紙“ 則很難歸類，因為它們大多是志工親手完成的。因此，我們將在今後的工作中進一步研究這兩個問題。

![[PaperWithCode/18-SMIE/Figure3.png]]
圖 3：NTU-120 資料集上 3 個隨機選擇的類別分割的混淆矩陣。 x 軸表示預測類別，y 軸表示真實類別。

![[PaperWithCode/18-SMIE/Figure4.png]]
圖 4：NTU-60 資料集上預測結果的可視化，其中每個類別的估計分數由相應的條形長度表示。最好以彩色形式觀看。

**SMIE 中對比學習的解釋**：全局對齊模組使用對比配對來估計視覺和語義特徵之間的互信息 [12, 26] 。它通過對齊 $p(v)$ 和 $p(a)$ 的兩個分佈來更好地連接視覺和語義空間。為此，聯合分佈 $p(v, a)$ 與邊際分佈 $p(v)p(a)$ 應該盡可能不同。因此，我們最大化 $v$ 和 $a$ 之間的互信息，這是 $p(v, a)$ 和 $p(v)p(a)$ 之間的 KL 散度，如公式 2 所示。對比學習用於最大化 KL 散度的近似估計值，如公式 4 所示。可以得出，$I(V , A) ≥ log(S) − LS$，其中
$$L_{S}=-E_{V}\left[\log\frac{f\left(v_{i},a_{i}\right)}{\sum_{x_{j}\in V}f\left(v_{j},a_{j}\right)}\right]. \tag{12}$$

這裡，$v_j$ 和 $a_i$（當 $j \neq i$ 時）是負樣本對，意思是視覺特徵 $v_j$ 和語義特徵 $a_i$ 不屬於同一動作類別。$S$ 代表所有負樣本對的數量。如果我們設定 $f=\exp\left(\frac{\|v_{i}-c\|_{2}^{2}}{\tau}\right),$，其中 $v_i$ 是視覺特徵，$c$ 是類別中心，$\tau$ 是一個控制分布銳度的溫度參數，那麼 $L_S$（對比損失）就等於這個公式。

總結來說，對比損失可以視為互信息的下界。在實驗中，對比學習被用來近似成對的視覺和語義特徵之間的互信息。在這種設置中，$(v, a)$ 是正樣本對，而 $(v', a)$ 是負樣本對。

## **5 結論**

在本文中，我們提出了一個基於骨架的互信息估計和最大化框架 (SMIE) 來解決零樣本動作識別問題。SMIE 包含一個全局對齊模組和一個時間約束模組。全局對齊模組通過應用互信息作為相似性度量來捕捉視覺空間和語義空間之間的複雜統計相關性。時間約束模組利用內在的時間信息來保持隨著觀察幀數的增加，互信息不斷增長。我們在三個骨架基準上進行了廣泛的實驗，包括 NTU-60、NTU-120 和 PKU-MMD 資料集，驗證了所提出的 SMIE 模型的有效性。