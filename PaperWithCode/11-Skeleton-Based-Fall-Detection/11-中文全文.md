#資料集/URFD #資料集/CAUCAFull #骨架偵測/YOLOv7-pose  #動作識別/演算法/LSTM   

摘要—跌倒是常見的健康問題，也是老年人嚴重受傷的主要原因之一。儘早檢測跌倒對於減少與跌倒相關的損害至關重要。雖然基於傳感器的跌倒檢測方法具有較高的檢測率，但可穿戴傳感器對使用者來說持續佩戴較為不便，而大型空間傳感器容易受到噪聲干擾且準確性較低。本文中，我們提出了一種基於視覺的跌倒檢測方法，該方法利用人體姿勢和長短期記憶（LSTM）網絡。我們從視頻中提取骨架數據並將其堆疊成序列，作為 LSTM 網絡的輸入特徵來分類跌倒活動。通過使用骨架數據，我們可以減輕環境因素（如光照變化和複雜背景）的影響。最終，我們提出的跌倒檢測方法在 URFD 和 CAUCA Fall 公共數據集上分別達到了 93% 和 97% 的高準確率。  
關鍵詞—跌倒檢測、智能醫療、姿勢估計、YOLOv7、長短期記憶網絡。

## I. 引言  
隨著現代醫學的進步，壽命不斷延長，導致全球人口老齡化現象。根據世界衛生組織（WHO）的數據，到 2030 年，全球每六人中就有一人年齡達到 60 歲或以上 [1]。因此，老年人的醫療需求變得越來越重要。美國疾病控制與預防中心（CDC）的統計數據顯示，在美國，約 28% 的 65 歲及以上的成年人每年都會報告發生跌倒，這導致約 800 萬起跌倒傷害需要醫療治療或至少限制活動一天 [2]。因此，為了及時提供醫療援助並最大限度地減少跌倒造成的傷害，針對老年人的跌倒檢測（FD）警報系統正在全球範圍內進行研究和開發。

雖然一般方法是使用傳感器檢測跌倒期間人的運動突然變化，但這些方法在技術和工程上存在許多差異。這些方法可以分為三類：基於可穿戴設備、基於環境感知和基於視覺的檢測系統 [3]。基於可穿戴設備的系統使用加速度傳感器安裝在人體上來檢測跌倒。許多商業化的智能設備如手環、手錶和項鍊已集成了這項技術。然而，這種方法的限制包括充電或佩戴的不便以及僅能檢測單人的跌倒。基於環境感知的 FD 系統使用光、運動、振動和接近傳感器在大範圍內檢測跌倒，但其準確性受到周圍環境的影響。與傳感器基礎系統相比，基於視覺的方法提供了許多優勢。最近的計算機視覺和機器學習的發展提高了基於視覺的 FD 系統的準確性。基於視覺的 FD 系統的挑戰包括隨著光照條件變化，準確性下降、背景複雜、許多日常活動看似像跌倒，還有訓練模型的數據不足。

我們提出的方法包含兩個主要階段。首先，我們利用精確的模型提取人體姿勢並將其編排成序列。經過多種姿勢估計模型的測試後，我們選擇了 YOLOv7-pose [4, 5] 來從輸入視頻中提取關鍵點。這些關鍵點包含了人體跌倒活動的重要特徵。在第二階段，這些提取的特徵被輸入到分類器中，以區分「跌倒」或「正常」的動作序列。為了處理視頻分類任務中的時間信息，我們決定使用 LSTM 模型作為分類器。

總結來說，我們提出的方法提供了以下主要貢獻：
- 我們的方法通過結合人體姿勢數據來減少環境因素的影響，這代表了跌倒檢測領域的一項重大進步。實驗結果表明，基於 YOLOv7 的姿勢估計比其他技術表現更為優異。
- 結合物體跟踪技術，檢測畫面中所有人的跌倒情況。我們使用了一種基於物體中心變化的簡單物體跟踪技術，確保整個系統能夠快速有效地處理數據。

本文的其餘部分結構如下：第二節介紹相關工作。第三節概述我們提出的方法，第四節則展示結果與評估。

## II. 相關工作  
跌倒檢測系統可分為三種類型：基於環境的、基於可穿戴設備的和基於視覺的。針對跌倒檢測已提出了多種方法，每種方法都有其優缺點。基於環境的方法使用光、運動、振動和接近傳感器來收集日常活動數據並檢測跌倒。這種方法的優點是成本低且方便用戶使用。然而，由於依賴於周圍環境，傳感器的準確性在某些情況下可能不精確，如光照變化或震動情況。相比之下，作者 [6] 提出的可穿戴傳感器方法，使用安裝在胸部和大腿上的兩個加速度計和陀螺儀。這些傳感器信號可用來處理人體的角速度和加速度，並通過閾值檢測可能導致跌倒的行為。儘管這種方法的準確性很高，但其限制包括可能會誤判跳上床或撞到牆等行為，且需要電池充電的不便會讓使用者感到不舒適。

如今，計算機視覺和機器學習已使基於視覺的跌倒檢測方法能夠取得更好的結果。例如，基於跌倒發生頻率低的情況，研究者 [7] 使用 3D 卷積網絡和自編碼器來檢測跌倒。其主要概念是讓神經網絡學習如何重建正常活動的整個視頻序列，當發生跌倒等異常行為時，重建的視頻與初始視頻之間會出現顯著差異。這種方法的優點是不需要精確標註數據，模型只需要正常的日常活動數據進行訓練，這些數據通常占大多數。然而，這種方法的限制在於可能會誤判其他異常行為，且處理時間和複雜計算可能不適合實時應用。

研究 [8] 使用定向梯度直方圖（HOG）算法來描述人體形狀，計算人體形狀的變化並結合支持向量機（SVM）分類器來進行跌倒檢測。該方法的局限在於模型的準確性取決於目標和背景的顏色差異。研究 [9] 使用光流算法將時間序列圖像轉換為一個捕捉所有動作的單一圖像，然後將生成的圖像通過卷積神經網絡來判斷輸入的時間序列數據是否被分類為「跌倒」或「正常」。這種方法的結果可以產生很高的準確性，但其缺點是光流算法只檢測動作，無論是衣物還是手提包等物體都會被檢測到。這一限制可以通過最近開發的姿勢識別算法來解決。

研究 [10] 結合了人體姿勢識別模型和 SVM 算法。具體來說，姿勢識別模型返回身體關鍵點的位置，並根據 SVM 算法對其進行跌倒分類。除了使用提取的關鍵點，研究 [11] 還計算了其他跌倒特徵，如身體速度、身體與地面的角度以及身體周圍的「邊界框」比例，這些特徵根據閾值進行分類。使用姿勢識別算法的優點在於它只關注人體物體，從而提高了模型的準確性。然而，研究 [10] 和 [11] 的限制是它們只使用一個幀進行識別，這可能會混淆躺臥和蹲伏等圖像。

研究 [12] 使用姿勢估計模型來提取關鍵點位置，之後將這些數據通過 LSTM 網絡進行行為分類。該模型由 MobilenetV2 網絡構建，以提高處理速度，並可分類七種室內姿態，包括躺臥、坐著、蹲伏、站立、行走、打鬥和跌倒。該模型的分類準確率高達 99%，但跌倒檢測的準確率為 83%。研究還顯示了時間序列分類在基於視覺的跌倒檢測中的潛力。

這些研究啟發我們開發了一個基於人體姿勢估計模型的跌倒檢測系統，該系統結合了人體幾何特徵和 LSTM 分類器。

## III. 提出的方法

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Figure1.png]]
圖 1. 系統概覽

### A. 系統概述  
我們提出的系統框圖如圖 1 所示。系統的工作流程如下：
- 輸入是一連串執行跌倒動作或其他日常活動的 RGB 圖像序列。這些畫面在訓練階段包含一個人的身體，在測試階段則包含多個人。
- 使用 
演算法提取骨架數據。
- 人體追蹤模塊在測試階段跟踪人物並堆疊他們的骨架數據。
- 每個人的關鍵點被堆疊成序列並進行歸一化處理。
- 這些序列通過 LSTM 分類器進行訓練和測試。
系統的輸出為帶有分類標籤（跌倒或正常）的人體骨架和邊界框。

### B. 數據集與預處理  
我們使用了三個數據集來訓練和評估模型：
- UR 跌倒檢測數據集（URFD）[13] 包含 70 個動作序列，涉及不同的主體和背景：30 個跌倒動作視頻和 40 個接近跌倒的日常活動視頻，如坐下、躺下和彎腰。
- CAUCA 跌倒數據集 [14] 包含 100 段視頻，具有遮擋、不同亮度水平、參與者、動作、背景和從相機到跌倒的距離等變量。數據集中有 50 段跌倒視頻和 50 段非跌倒活動視頻。
- 我們還收集了額外的數據，包括來自 YouTube 和其他來源的跌倒視頻，以增加訓練數據集的多樣性。

收集的數據包含了多種場景、主體和拍攝角度。圖 2 顯示了每個數據集的示例。為了增加訓練數據量，我們將每個視頻分為 60 幀連續圖像集，對應於持續 2-3 秒的動作序列。

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Figure2.png]]
圖 2. 資料集範例 (a) CAUCA 跌倒資料集 (b) UR Fall 資料集 (c) 我們收集的跌倒資料集

### C. 人體姿勢估計  
姿勢識別由 YOLOv7-pose 完成，該模型是一個開源的多人體姿勢估計模型。YOLOv7-pose 是從 YOLO-pose [4] 和 YOLOv7 [5] 框架開發而來，專用於人體姿勢估計。YOLOv7-pose 是一個單階段多人體關鍵點檢測器，能同時檢測多個人的邊界框和關鍵點。輸入為 RGB 圖像，輸出包含邊界框關鍵點的位置和置信度。關鍵點的置信度代表關鍵點的可見性，當關鍵點可見或被遮擋時，該值接近 1，當關鍵點位於視野之外時，該值接近 0。通過 17 個關鍵點的預測，每個錨點的預測向量如下定義：
  
$$P_v=\{C_x,C_y,W,H,Conf_{box},Conf_{class},K_x^1,K_y^1,K_{conf}^1,...\\,...K_x^{17},K_y^{17},K_{conf}^{17}\} \tag{1}$$

YOLOv7 的速度和準確性提升使得 YOLOv7-pose 在多人體姿勢估計模型中取得了更好的結果。圖 3 顯示了 YOLOv7-pose 與 OpenPose [15] 的比較，YOLOv7-pose 在跌倒過程中能以適當的置信度捕捉所有關鍵點。

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Figure3.png]]
圖 3 YOLOv7-Pose 與其他姿態估計模型的比較 (a) 輸入影像 (b) OpenPose 的輸出 (c) YOLOv7-Pose 的輸出

### D. 數據處理  
每個人的原始數據在追蹤後會進行處理，然後再傳送到分類器。該處理過程包含三個主要步驟：
- 第一步（過濾）：YOLOv7 的輸出包含 57 個參數，包括邊界框、關鍵點的坐標和置信度分數。在處理過程中可能會丟失某些關鍵點。然而，由於 YOLOv7 的姿勢估計具有高精度，我們可以通過選擇較低的關鍵點置信度閾值來處理關鍵點丟失的問題。當有足夠多的關鍵點時，必要的數據幀只包括邊界框和關鍵點的坐標與位置。
- 第二步（縮放中心）：身體的初始位置可能位於畫面中的任意位置。為了獲得最佳質量的特徵，我們將上述數據幀轉換為所有檢測對象的中心位置。
- 第三步（歸一化）：關鍵點和邊界框的位置會按圖像的高度和寬度進行縮放。為了提高模型的準確性並減少模型的收斂速度，我們將數據歸一化到區間 [-1, 1]。

### E. 分類器與訓練  
跌倒是一個連續的動作序列，檢測任務需要同時考慮空間和時間信息。如圖 4 所示，我們使用多對一的 LSTM 網絡來檢測跌倒活動。LSTM 是由循環神經網絡（RNN）發展而來的——這是一種可以處理序列和時間序列數據（如文本、音頻和視頻）的神經網絡。RNN 將前一狀態中的部分信息攜帶到下一個階段，並最終將它們結合起來以預測結果。然而，攜帶過多信息可能會導致梯度消失，從而導致信息丟失。為避免長期依賴問題，LSTM 通過添加更多的計算閘門來控制每個狀態中隱藏多少信息以及輸出哪些信息。

我們提出的 LSTM 網絡輸入來自連續幀中提取的 60 個特徵。在移除 YOLO-pose 的置信度分數後，每個特徵包含 38 個參數。我們將每組 60 個特徵標記為 0 或 1，分別代表非跌倒和跌倒結果。

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Figure4.png]]
圖 4. 用於跌倒偵測的多對 LSTM 模型。
## IV. 結果與評估  
我們提出的方法的輸出包含每個畫面中每個人的位置和預測結果（跌倒/正常）。圖 5 顯示了模型結果及其對應的輸入序列。與其他分類模型一樣，我們使用混淆矩陣（如表 1 所示）進行評估，並關注精確率、召回率和準確率。我們還使用靈敏度和特異度來與其他方法進行比較。評估標準計算如下：

$$Precision=\frac{TP}{TP+FP}$$

$$Recall (Sensitivity)=\frac{TP}{TP + FN}$$
 
$$Specificity=\frac{TN}{TN + FP}$$ 
$$Accuracy=\frac{TP + TN}{TP + TN + FP + FN}$$


![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Table1.png]]
表 1. 跌倒偵測的混淆矩陣

我們的方法在兩個不同數據集 URFD 和 CAUCA 上的評估結果如表 2 所示。這些數據集中的每段視頻都描繪了一個物體的動作，並從不同的相機角度捕捉。模型在這些由多位具有不同身體特徵的參與者生成的數據集上的評估強調了其在多樣化真實場景中的出色表現。

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Table2.png]]
表 2. 我們的方法在測試資料集上的結果

數據顯示，我們的模型在 CAUCA 數據集上的表現優於 URFD 數據集。造成這一差異的主要原因之一可以歸因於圖像捕捉過程中的相機角度及各數據集的主要目標。CAUCA 數據集於 2022 年構建，其主要目的是通過計算機視覺解決跌倒預測問題，並使用斜角相機進行拍攝，圖像質量比 2014 年捕獲的 URFD 數據集要好。CAUCA 數據集中的動作通過安裝在房間頂部的側視相機進行了全面記錄，如圖 5b 所示，這使得模型的預測能力非常優越。

另一方面，URFD 數據集於 2014 年錄製，旨在用於基於視覺和傳感器的各種方法。該數據集使用了 RGB 和深度相機以及傳感器數據進行捕捉。URFD 數據集的相機角度相對於物體的運動是水平的，這使得準確捕捉運動變得困難，特別是像向前或向後跌倒這樣的動作。因此，預測模型在 URFD 數據集上的表現不如 CAUCA 數據集。

由於 CAUCA 數據集於 2022 年才推出，目前很少有研究使用該數據集。為了將我們的模型與其他方法進行比較，我們使用 URFD 數據集評估了特異度和靈敏度（召回率）兩項指標。儘管我們的方法沒有在這些指標上超越之前的研究，但它能夠在包含多個個體的幀中進行應用和估算。

關於在多個人的畫面中識別跌倒，我們的方法通過跟蹤每個人在幾個連續幀中的運動並為畫面中的每個人生成預測結果。因此，該方法的結果不僅預測了畫面中個體的跌倒行為，還識別了他們的跌倒位置。模擬結果如圖 5c 所示。該模型在包含多個人的視頻幀上運行的能力強調了其實際應用性，這一點是表 3 中所列研究尚未實現的。

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Table3.png]]
表 3. 我們的結果與其他方法在 URFD 資料集上的比較

![[PaperWithCode/11-Skeleton-Based-Fall-Detection/Figure5.png]]
圖 5 測試資料集跌倒偵測結果 (a) URFD 資料集結果 (b) CAUCA 資料集結果 (c) 多人跌倒偵測

## **結論**  
本文提出了一種基於視覺的跌倒檢測方法，該方法依賴於人體骨架。我們的方法使用 YOLO-Pose 進行姿勢估計，並使用 LSTM 來識別跌倒活動。該系統能夠在多種相機角度下運行，並在 URFD 數據集上達到 93.3% 的準確率，在 CAUCA 數據集上達到 97.0% 的準確率。更具體地說，我們的方法能夠在包含多個人的畫面中檢測和定位跌倒活動，這強調了其在真實場景中的實際應用性。