
摘要 — 人類行為識別是許多人機互動應用的核心驅動力。目前大多數研究專注於通過整合多種同質模態（如 RGB 圖像、人體姿勢和光流）來提升模型的泛化能力。此外，場景類別和人本身對上下文交互和脫離上下文的手語進行了驗證。這些將外觀特徵和人體姿勢結合的嘗試已經展示了積極的成果。然而，由於人體姿勢的空間誤差和時間模糊性，現有方法存在可擴展性差、穩健性有限以及模型次優化的問題。在本文中，受到不同模態可能保持時間一致性和空間互補性的假設啟發，我們提出了一種新穎的雙向同時與跨空間注意力融合模型（B2C-AFM）。我們的模型特點是多模態特徵在時間和空間維度上的異步融合策略。此外，我們還探索了新穎的明確運動導向的姿勢表示，稱為肢體流場（Lff），以緩解人體姿勢的時間模糊性。在公開數據集上的實驗驗證了我們的貢獻。豐富的消融研究實驗表明，B2C-AFM 在已知和未知的人類行為中均表現出穩健的性能。代碼可在 [https://github.com/gftww/B2C.git](https://github.com/gftww/B2C.git) 獲得。  

**關鍵詞** — 人類行為識別，同質模態，融合模型，肢體流場，B2C-AFM。


## **I. 引言**

基於視覺驅動的人類行為識別 (HAR) 隨著深度神經網絡的繁榮取得了顯著進展，並逐漸應用於各種領域，如視頻監控、自動駕駛和社交機器人。HAR 的核心任務是在視頻中將某一特定的人類行為分類為預定義的動作類別，例如「脫帽」、「摘下眼鏡」、「拍手」和「揮手」等。考慮到人類行為的無約束性，像「帽子」和「眼鏡」這樣的場景對象類別似乎對正確識別上下文交互是必要的。相反，這些對象在識別脫離上下文的手語時會成為干擾噪聲，例如「拍手」、「揮手」等。在這種情況下，根據需要自適應選擇不同的模態作為輸入特徵來推斷人類行為是合理的，這些模態可以用來表示上下文場景、人體、時間流等。

現有的行為識別研究主要分為基於外觀的方法和基於姿勢的方法，並且已經在研究一些同質模態如 RGB 圖像、光流和人體姿勢方面取得了積極成果。外觀序列，即 RGB 圖像，可以提供關於場景類別的豐富紋理、顏色和空間信息，因此廣泛被選作基於外觀方法的主要模態。SlowFast 網絡充分探索了高頻率和低頻率的外觀序列，並在視頻中的動作檢測和分類中取得了令人印象深刻的效果。為了正確解釋場景，Xue 等人引入了 Transformer 來提取視覺標籤和外觀類別，這些可以成為預測前瞻性行為的適當特徵。此外，一些大型、標註良好且公開可用的人類行為數據集由 RGB 圖像描述，這些預定義的動作類別表明了外觀序列的豐富表達空間，例如 ActivityNet、Kinetics 和 NTU-RGBD。然而，基於外觀的方法容易受到視角變化、背景和光照條件變化的影響。它們很容易被脫離上下文的行為所誤導。在這種情況下，僅依賴外觀序列的模型很難在已知和未知的行為中實現穩健的推理性能。

最近的研究嘗試加強人類運動的表示，這可以更清晰地區分前景。由於人體姿勢聚焦於動作的特性和較低的計算成本，人體姿勢極大地促進了基於姿勢的方法。人體姿勢，即由 2D/3D 關節坐標組成的骨架，用於將人體動作簡化為圖形拓撲。它的緊湊性使其不受上下文干擾、光照變化和背景變化的影響。儘管如此，由於缺乏顯式的序列順序來建模人類動作，這可能導致時間上的模糊性，例如「站起來」和「坐下來」這兩個動作。空間-時間圖卷積網絡（ST-GCN）創新性地引入了空間和時間邊緣來建模動態骨架。圖卷積網絡（GCNs）隨後成為提取各種人體動作圖形的標準方法。接下來，一種新穎的譜圖卷積網絡被引入來學習卷積參數和拉普拉斯運算符。值得注意的是，Hichem Sahbi 提出了多拉普拉斯 GCN，通過譜域中的凸組合學習圖形拉普拉斯。可學習的連通性被驗證有助於提取圖形的拓撲屬性，並且詳細實驗表明這些圖形相比手工製作的圖形具有優越性。然而，時間序列中估計或捕獲的人體姿勢仍然存在準確性不足和穩健性差的問題。

在實際應用中，現實場景中的「複雜和多樣的背景」、「多主體交互和群體活動」以及「長距離和低質量視頻」帶來了更大的挑戰。Xu 等人提出了一種新穎的時間擠壓空間對比損失，並在三個層次上對運動特徵進行對比。金字塔聚合注意力以逐層方式突出補充的語義信息。儘管這些優秀嘗試取得了一些成果，但人體骨架的微小擾動可能導致行為識別的失敗。此外，人體骨架的非規則圖形很難與其他模態融合。Simonyan 和 Zisserman 引入了密集光流來捕捉運動中的補充信息。光流通常專注於描述相鄰幀之間的時間變化，而基於視覺詞彙的顯式運動建模被驗證是穩健的。然而，多幀密集光流往往會消耗大量計算資源，並且這些不加區別的前景和背景位移場無法突出人體行為。

典型地，IntegralAction 將人體姿勢建模為關鍵點熱圖（Kh）和部件關聯場（PAF）的結合，並通過姿勢門控的特徵整合來減輕融合過程中對外觀流的強烈偏倚。相比之下，PoseConv3D 引入了肢體熱圖（Lh），並採用雙向 3D 卷積神經網絡來融合 RGB+姿勢模態，在公開基準上取得了令人印象深刻的精度。然而，不同動作的持續時間不同，即使是相同的動作由不同的人執行也會導致時間變化。實際上，每個行為視頻的約 8-32 幀會從超過 300 幀的視頻中被採樣來預測類別分數。因此，稀疏幀和隨機變化導致的時間模糊性似乎是不可避免的，這加劇了推斷模型的穩健性問題。為了突出人體運動的前景和執行順序，我們認為高效地探索以運動為導向的姿勢表示是必要的。據我們所知，現有方法缺乏對人體姿勢進行顯式時間建模。此外，同質模態的缺陷融合策略進一步限制了推斷模型的可擴展性。

![[PaperWithCode/14-B2C/Figure1.png]]
圖1 時間一致性與空間互補性示意圖。以「打電話」這個動作為例，每對人體姿勢和外觀圖像在時間上具有相同的節奏，在空間上補充人-物體特徵：（a）同時注意力，（b）跨空間注意力。符號「⊗」和「⊕」分別表示逐元素乘法和加法。

本文的動機來自於多模態在時間和空間上的各向異性。如圖 1 所示，我們以「打電話」這一行為為例。首先，作為一個時間過程，人體運動大致經歷了三個階段：準備、核心和撤退。假設綠色虛線表示時間注意力，人類姿勢和外觀圖像往往共享相同的時間節奏。設計一個同步時間注意力機制可以確保不同模態之間的時間一致性。其次，為了識別上下文行為，人體姿勢和外觀圖像在空間上往往是互補的。如圖 1(b) 所示，正確識別「打電話」有必要提供有關人體和電話的位置信息。設計一個跨空間注意力機制可以集成人體姿勢和外觀圖像的特徵，從而遵循空間互補性。基於這一觀察，我們假設同質模態保持時間一致性和空間互補性。如圖 2 所示，我們提出了一種新穎的雙向同步時間和跨空間注意力融合模型（B2C-AFM）來進行人類行為識別。該方法的整體流程包括三個主要特徵：將外觀和姿勢序列作為雙流網絡的輸入；以運動為導向的肢體姿勢表示被堆疊到姿勢序列中；融合模塊採用沿時間和空間維度的異步策略。具體來說，我們的主要貢獻可以總結如下：

A. **多模態驅動方法**  
我們提出了一種新穎的多模態驅動方法 B2C-AFM，將外觀、關節熱圖和肢體流場作為輸入。其雙流管道及融合模塊旨在從外觀與姿勢特徵的交互中學習動態模式。  

B. **以運動為導向的姿勢表示**  
為了明確建模人體運動，我們探索了一種名為肢體流場（Lff）的新穎運動導向姿勢表示。在姿勢表示上的消融研究表明，Lff 有助於緩解由於稀疏採樣導致的時間模糊性。  

C. **異步融合策略**  
對於融合模塊，我們引入了同步時間尺度單元和跨空間融合單元，依次沿時間和空間維度融合雙流特徵。這種異步融合策略強調了同質模態在同一行為中的時間一致性和空間互補性。  

D. **模型泛化能力**  
在公開基準上的實驗驗證了我們方法的競爭性能和貢獻。我們對我們貢獻的系統性消融實驗證明了該方法在已知和未知人類行為的推斷能力上的優勢。

![[PaperWithCode/14-B2C/Figure2.png]]
圖 2.我們方法的整個流程。符號「⊗」和「⊕」分別表示逐元素乘法和加法。

我們論文的其餘部分組織如下：我們首先討論第二節中的相關工作。 II，我們的姿勢表示和肢體流場方法的概述在第二節中介紹。三、我們在第二節中介紹了融合模組和動作分類。 IV 和 Sec。分別為V；這些相應的實施細節遵循第 2 節中的要求。六；之後，我們在第二節報告實驗。 VII，結論在第 2 節提出。

## **II. 相關工作**

### **A. 基於多模態的行為識別**  
為了提升模型在不同交互場景中的泛化能力，最近的人類行為識別研究試圖利用多種模態。在這些模態中，時間運動和人體姿勢通常被視為外觀序列的補充模態。一方面，Simonyan 和 Zisserman 首次提出了雙流網絡來結合 RGB 圖像和密集光流。ConvNet 進一步研究了不同的空間和時間融合策略，實驗指出在卷積層中的後期融合比早期模塊中的融合效果更好。另一方面，IntegralAction 提出了基於姿勢的特徵整合，而 2D 人體姿勢在識別未知的人類動作中顯示出了優勢。PoseConv3D 比較了 2D 和 3D 骨架的識別性能，發現 3D-CNN 模型依賴於外觀和姿勢的體積，而 2D 人體姿勢似乎更具優勢。此外，還有一些異質模態，包括標籤、活動知識和音頻等，這些卓越的工作證實了多模態對行為識別的重要性。與上述模態不同，我們專注於探索運動導向的姿勢表示。人體姿勢表現出與外觀一致的分佈，這種特性可以導致顯式的時間對齊和空間融合策略。

### **B. 姿勢表示**  
考慮到人體姿勢，現有的研究大致可分為三類：基於圖的、基於熱圖的和基於向量場的表示。最初，ST-GCN 提出了人體姿勢序列的時空圖，用於從數據中學習動態模式。與靜態的手工圖形相比，動態 k-近鄰圖在點雲中進行動態連接。儘管基於圖的表示具有表現力，它們的不規則圖形使得它們難以與常規網格上的圖像模態進行顯式對齊和融合。實際上，每個關節的 2D 高斯熱圖已被廣泛驗證對於姿勢估計是有用的，甚至 1D 解耦向量也能取得令人信服的效果。此外，IntegralAction 和 PoseConv3D 中的積極嘗試展示了基於熱圖的表示的融合優勢。2017 年，Cao 等人創新性地提出了部件親和場（Part Affinity Fields），這是一種可以聯合連接部件位置的向量場。時間綜合關聯場可以表示同一像素位置處兩個關節之間的零距離關聯。隨後，人體姿勢的運動嵌入迅速發展，包括 PoTion、JointFlow 和肢體的時間流映射。然而，現有的運動嵌入高度依賴於粗略估計的關節和肢體軌跡。尤其是時間流映射將肢體運動場簡化為一對時間相鄰肢體段之間的區域，忽略了肢體運動的常見模式。在這種情況下，我們引入並簡化了相鄰幀中肢體運動的基本形式，主要包括平移、旋轉和翻轉。

### **C. 融合模塊**  
現有的融合策略主要涉及兩個概念：決策層融合和特徵層融合。前者直接平均每個子模型預測的分類分數，這可以視為一種集成學習，用於測試不同模態的置信度。決策層融合策略能夠適應某些模態缺失的情況，但無法從多模態特徵中構建聯合表示。相比之下，特徵層融合方法能夠充分考慮各種模態之間的相關性和交互。簡單的融合方法包括加權求和、串聯和線性調制等，這些是基本操作。此外，注意力機制被廣泛引入，以學習融合過程中的動態權重。各種基於注意力的融合方法如雙向傳播策略、門控整合和 3D 卷積融合等已被提出。為了自適應地強調多模態之間的相關特徵，共同注意力和序列通道-空間框架被驗證為有幫助。基於現有融合方法的研究，我們設計了一種新穎的異步融合策略，先進行時間縮放，然後進行空間聚合。

## **III. 方法概述**

在本文中，人類行為識別指的是模式分類，該分類涉及將動作視頻片段的模式分類到預設的類別中，這些類別在不同場景中的具體含義可能有所不同。

### **A. 問題公式化**  
給定一個動作視頻片段，我們的方法 B2C-AFM 將外觀序列 A 和姿勢序列 P 作為輸入。外觀序列指的是連續的 RGB 幀。我們將$\mathbf{A} \in \mathbb{R}^{T\times C_A\times H_A\times W_A}$ 重新格式化，首先隨機選擇第一幀，然後從動作視頻片段中每隔 $\mathcal{T}$ 幀收集後續幀。相應地，可以根據每個已採樣的外觀幀中的人體姿勢構建姿勢序列 $\mathbf{P}\in\mathbb{R}^{T\times C_{\mathbb{P}}\times H_{\mathbb{P}}\times W_{\mathbb{P}}}$。符號 $T$、$C_*$、$H_*$ 和 $W_*$（其中 $∗ ∈ {A, P}$）分別表示序列的幀數、通道數、高度和寬度。具體來說，所謂的人體姿勢由身體關節$J_p^t=\left(x_j,y_j,c_j\right)|_{j=1}^{j=\mathcal{J}}$  組成，其中  $p\in[1,\mathcal{P}],$，包括第 $p^{th}$ 個人在 $t^{th}$ 幀中第 $j^{th}$ 個關節的 2D 坐標 $(x_j , y_j)$ 和置信度 $c_j$。$\mathcal{P}$ 和 $\mathcal{J}$ 分別表示人數和關節數。為了與其他工作進行公平比較，我們使用了公共數據集（如 Kinetics、Mimetics 和 NTU-RGBD）中提供的人體姿勢數據。

要解決人類行為識別問題，數據驅動的學習方法將管道分為三個步驟：姿勢表示、特徵提取和動作分類。第一步是合理建模姿勢序列作為輸入；第二步是對外觀和姿勢序列進行特徵提取，基於深度神經網絡可以構建雙流提取器。最後，將這兩個提取的特徵結合成綜合特徵，進行動作分類，輸出目標類別 $\mathcal{C}$ 的動作概率。

### **B. 管道概述**  
本文中的整個管道由姿勢表示、特徵提取器、融合模塊和動作分類組成。如圖 2 所示，外觀和姿勢序列均作為輸入，採用由 AppearanceNet 和 PoseNet 組成的雙流提取器分別獲取外觀特徵 $F_A \in \mathbb{R}^{T \times C_A^f \times H_A^f \times W_A^f}$ 和姿勢特徵 $F_P \in \mathbb{R}^{T \times C_P^f \times H_P^f \times W_P^f}$。提取出的雙流特徵將被融合模塊縮放和融合，該模塊在不改變其對應維度的情況下分別輸出雙流特徵。之後，設置全局平均池化層 (GAP) 對外觀和姿勢特徵的空間大小進行歸一化。歸一化後的特徵可以沿著通道維度串聯，從而計算最終的動作得分。對於 AppearanceNet 和 PoseNet 的架構，我們採用了 ResNet 和時間偏移模塊 (TSM) 進行特徵提取，這與大多數相關工作一致。考慮到 AppearanceNet，它的網絡架構和由 RGB 幀組成的輸入序列已經相對成熟。因此，本文的其餘部分將重點介紹姿勢表示、融合模塊和動作分類。

### **C. 姿勢表示**  
人體姿勢由身體關節 $J_p^t$ 組成，這些關節指的是圖像坐標系中離散的像素位置。為了更好地與外觀流整合，現有方法嘗試將人體姿勢重新格式化為基於規則網格的類似圖像的同質模態，例如關鍵點熱圖、肢體熱圖和部件親和場。其中，關鍵點熱圖使用高斯斑點將關鍵點轉換為偽熱圖：

$$Kh_j^t=\exp\left(-\frac{\left(x-x_j\right)^2+\left(y-y_j\right)^2}{2*\sigma^2}\right)*c_j, \tag{1}$$

其中，關節 $(x_j , y_j , c_j) \in J_p^t$，並且 $Kh \in \mathbb{R}^{T \times \mathcal{J} \times H_P \times W_P}$ 通過堆疊所有採樣幀中的關節熱圖 $Kh_j^t$ 得到。高斯圖的方差 $σ$ 設為 1。如果檢測到多個人在幀中，我們會將具有最高置信度的前 5 個人的熱圖相加。否則，當人數少於 5 人時，將使用零填充張量補充熱圖。類似地，人體肢體的偽熱圖可以表述為：

$$Lh_j^t=\exp\left(-\frac{\mathcal{D}\left(\left(x,y\right),limb\left[s_k^t,e_k^t\right]\right)^2}{2*\sigma^2}\right)*c_k, \tag{2}$$

其中，函數 $\mathcal{D}$ 用於測量點 $(x, y)$ 到肢體 $[s_k^t, e_k^t] \in N_{limb}$ 的距離，且 $c_k$ 表示點 $s_k^t$ 的置信度。這裡，從人體骨架中採樣的肢體集合可以表示為：

$$N_{limb}=\left\{\left[s_k^t\left(x_i,y_i\right),e_k^t\left(x_j,y_j\right)\right]|_{k=1}^{k=\mathcal{K}}\right\}, \tag{3}$$

其中，第 k 個肢體中的點 $s_k^t$ 和 $e_k^t$ 都屬於 $J_t^p$，而 $\mathcal{K}$ 表示每個人的肢體數量。在這種情況下，肢體熱圖 $Lh \in \mathbb{R}^{T \times \mathcal{K} \times H_P \times W_P}$ 可以通過堆疊所有採樣幀中的肢體集合的熱圖 $Lh_t^j$ 來生成。


### **D. 肢體流場**

![[PaperWithCode/14-B2C/Figure3.png]]
圖3.肢體運動的研究。白色和棕色骨骼分別代表目前幀和下一幀中的人體姿勢。每個子圖中的彩色箭頭簇突顯了人體四肢掃過的運動場。我們可以觀察最常見的肢體運動：（a）平移，（b）繞重合端點旋轉，（c）繞交點端點旋轉，（d）繞交點旋轉。

與上述關於關節和肢體的空間表示不同，肢體流場（Limb Flow Fields, Lff）旨在顯式建模時間運動場。如圖 3 所示，最常見的肢體運動可以分解為平面平移和旋轉。針對相鄰幀中第 $k^{th}$ 個肢體的運動，我們將其運動場的速度向量公式化如下：

$$\begin{aligned}&Lff_{k}^{t}\\&=\begin{cases}\vec{v}_{s}*\ominus_{{D_{o},s_{k}^{t},s_{k}^{t+1}}}+\vec{v}_{e}*\ominus_{{D_{o},e_{k}^{t},e_{k}^{t+1}}},&if\quad\exists D_{o},\\\vec{v}_{s}*\Box_{{s_{k}^{t},e_{k}^{t},\tilde{e}_{k}^{t+1},s_{k}^{t+1}}}+\vec{v}_{e}*\ominus_{{s_{k}^{t+1},\tilde{e}_{k}^{t+1},e_{k}^{t+1}}},&elif\mathrm{~}\exists P_{o},\\\vec{v}_{m}*\Box_{{s_{k}^{t},e_{k}^{t},e_{k}^{t+1},s_{k}^{t+1},}}&others.&&\end{cases}\end{aligned} \tag{4}$$

其中，$Lff_{k}^{t}$ 的數量總共有 $T-1$ 幀，$D_o$ 表示 $limb[s_k, e_k]$ 和 $limb[s_{k+1}, e_{k+1}]$ 的交點，$P_o$ 表示速度方向段 $\vec{v}_s$ 和 $\vec{v}_e$ 的交點。運算符 $\square$ 和 $\ominus$ 分別表示四邊形和扇形區域的遮罩。在區域遮罩內的點 $(x, y) \in \mathbb{R}^{H_P \times W_P}$ 的像素值設為 1，其他條件設為 0。例如，四邊形 $\square_{s_k^t,e_k^t,e_k^{t+1},s_k^{t+1}}$ 由點 $s_k^t,e_k^t,e_k^{t+1}$ 和 $s_k^{t+1}$ 逆時針圍繞。扇形 $\ominus_{D_o,s_k^t,s_k^{t+1}}$ 以 $D_o$ 為原點，並以 $s_k^t$ 和 $s_k^{t+1}$ 作為扇形旋轉的起點和終點。 

如圖 4(a) 和 (c) 所示，最常見的肢體運動是旋轉和平移，這可以通過是否存在交點 $D_o$ 來區分。特別是當採樣幀數較少時，肢體的過度旋轉和平移會導致如圖 4(b) 所示的翻轉運動。在這裡，我們將翻轉運動分解為平移 $\square_{s_k^t,e_k^t,\tilde{e}_k^{t+1},s_k^{t+1}}$ 和旋轉 $\Theta_{s_k^{t+1},\tilde{e}_k^{t+1},e_k^{t+1}}.$。臨時點 $\tilde{e}_k^{t+1}$ 可以通過沿 $\vec{v}_s$ 的方向平移 $e_k^t$ 來得到，相關的速度變量可以表示如下：

$$\begin{cases}\vec{v}_s=s_k^{t+1}-s_k^t,\\\vec{v}_e=e_k^{t+1}-e_k^t,\\\tilde{e}_k^{t+1}=e_k^t+\vec{v}_s,\\\vec{v}_m=\vec{v}_s+\vec{v}_e,\end{cases} \tag{5}$$\

其中，$\vec{v}_m$ 表示 $\vec{v}_s$ 和 $\vec{v}_e$ 的合速度。與關鍵點熱圖類似，我們通過堆疊肢體運動 $Lff_k^t$，將肢體流場 $Lff' \in \mathbb{R}^{(T-1)\times(2\times\mathcal{K})\times H_{P}\times W_{P}}$ 重塑。此外，$Lff'$ 中的每個速度向量都被歸一化為單位向量，其分量位於 x 和 y 方向上，對應於 $2K$ 維度。我們將所有肢體運動的平均值視為背景，並通過將該背景堆疊到 $Lff'$ 的頂部來建模肢體流場 $Lff \in \mathbb{R}^{T\times(2\times\mathcal{K})\times H_{P}\times W_{P}}$。

![[PaperWithCode/14-B2C/Figure4.png]]
圖4.肢體運動示意圖。給定第k個肢體段 $limb[sk , ek]$，肢體運動場被示出為由點 $s_k^{t+1}$、$e_k^{t+1}$ 、$s_t^k$ 和 $e_t^k$ 圍繞的陰影區域。陰影中的像素點共享相同的速度向量  $\vec{v}(x,y)$ 肢體運動的常見情況可簡化為： (a) 與位移交點 $D_o$ 的旋轉； (b) 與方向交點 $P_o$ 翻轉； (c) 無交集的平移。

## **IV. 融合模塊**

為了整合外觀特徵 $F_A$ 和姿勢特徵 $F_P$，我們在本節中探討了雙向融合單元，具體包括雙向融合單元、門控融合單元、共同時間尺度單元和跨空間融合單元。卷積層（帶或不帶轉置）將執行上採樣或下採樣卷積，可將源特徵的維度重塑為目標維度。我們在這裡設計了一個卷積塊（CB）作為基本結構，它由一個卷積層、一個批量正則化層和激活函數（如 Sigmoid, ReLu 等）組成。例如，3D-CB-Sigmoid 代表由一個 3D 卷積層和一個 3D 批量正則化層與 Sigmoid 激活函數組成的組合。

![[PaperWithCode/14-B2C/Figure5.png]]
圖 5. 基本融合單元。 (a) 雙向融合單元，(b) 門控融合單元。

### **A. 基本融合單元**  
參考 SlowFast 和 IntegralAction，雙向架構和門控注意機制已被驗證是有用的。為了將外觀特徵 $F_A$ 和姿勢特徵 $F_P$ 融合到 $F'_A$ 和 $F'_P$，我們列舉了雙向融合單元和門控融合單元，如圖 5 所示。雙向融合單元（BFU）基於雙向架構，分兩步進行：（1）對齊，（2）聚合。特徵 $F'_A$ 和 $F'_P$ 的計算如下：

$$\begin{cases}\mathbf{F}_\mathrm{A}^{\prime}=\mathbf{F}_\mathrm{A}+CAB_\mathrm{A}\left(\mathbf{F}_\mathrm{P}\right),\\\mathbf{F}_\mathrm{P}^{\prime}=\mathbf{F}_\mathrm{P}+CAB_\mathrm{P}\left(\mathbf{F}_\mathrm{A}\right),&&\end{cases} \tag{6}$$

其中，$C_{AB_*}$ 表示卷積對齊塊（CAB），其目的是保持每個融合特徵的維度不變。這裡的 $C_{AB_*}$ 網絡結構指的是 $3D-CB-ReLu$。

門控融合單元（GFU）旨在通過執行門控注意機制學習 $F_A$ 和 $F_P$ 之間的互補特徵，如下所示：

$$\left.\begin{cases}\mathbf{F}_\mathrm{A}^{\prime}=\mathbf{F}_\mathrm{A}\odot g_\mathrm{A}+CAB_\mathrm{A}\left(\mathbf{F}_\mathrm{P}\right)\odot\left(1-g_\mathrm{A}\right),\\\mathbf{F}_\mathrm{P}^{\prime}=\mathbf{F}_\mathrm{P}\odot g_\mathrm{P}+CAB_P\left(\mathbf{F}_\mathrm{A}\right)\odot\left(1-g_\mathrm{P}\right),&&\end{cases}\right. \tag{7}$$\

其中，$g_A$ 和 $g_P$ 表示門控權重，範圍在 [0, 1] 之間：

$$\begin{cases} g_\mathrm{A}=CGB_A\left(\mathbf{F_P}\right),\\ g_\mathrm{P}=CGB_P\left(\mathbf{F_A}\right),\end{cases} \tag{8}$$\

其中，$C_{GB_*}$ 表示卷積門控塊（CGB），其網絡結構基於 3D-CB-Sigmoid。$C_{AB_*}$ 設置為 $3D-CB-ReLu$，兩者的區別在於激活函數。在本文中，我們將雙向融合單元視為基準。雙向和門控融合單元採用了同步的時空融合策略，忽略了多模態的各向異性。

![[PaperWithCode/14-B2C/Figure6.png]]
圖6.融合單元包含同時域尺度單元（左）與跨空間融合單元（右）。 「擴展」操作是指沿著通道維度複製並填滿選通權重。
### **B. 共同時間尺度單元 (Co-Temporal Scale Unit, CSU)**

為了全面研究時間一致性和空間互補性，我們將融合模塊分解為共同時間尺度單元 (CSU) 和跨空間融合單元 (CFU)。對中間特徵的通道和空間維度進行縮放操作，可以實現一致的改進。這裡，我們的目標是以相同的時間節奏縮放外觀和姿勢特徵的時間維度。CSU 被設計用來學習這兩個流特徵的共享時間注意力。為了突出時間節奏的重要性，$F_A$ 和 $F_P$ 會通過對其空間維度進行平均池化來縮小為 $\mathbf{F}_\mathrm{A}^t\in\mathbb{R}^{C_\mathrm{A}^f\times T\times1}\mathrm{~and~}\mathbf{F}_\mathrm{P}^t\in\mathbb{R}^{C_\mathrm{P}^f\times T\times1}$。如圖 6 所示，CSU 定義如下：

$$\begin{cases}\mathbf{F}^{co}=\text{Co-encoder}\left(\left[\mathbf{F}_{\mathrm{A}}^{t},\mathbf{F}_{\mathrm{P}}^{t}\right]\right),\\h_{\mathrm{A}}=\text{Decoder}_{\mathrm{A}}\left(\mathbf{F}^{co}\right),\\h_{\mathrm{P}}=\text{Decoder}_{\mathrm{P}}\left(\mathbf{F}^{co}\right).&&\end{cases} \tag{9 }$$

其中，我們將 $F_A^t$ 和 $F_P^t$ 串聯作為 Co-encoder 的輸入，用於學習共現特徵空間。Co-encoder 的網絡結構參考 $2D-CB-ReLu$，並計算出共現特徵 $\mathbf{F}^{co} \in \mathbb{R}^{C^{co}\times T\times1\times1}$，以及通過 $Decoder_A$ 和 $Decoder_P$ 計算出的縮放權重 $h_\mathrm{A}\in\mathbb{R}^{C_\mathrm{A}^f\times T\times1\times1}$ 和 $h_\mathrm{P}\in\mathbb{R}^{C_\mathrm{P}^f\times T\times1\times1}$。與 Co-encoder 相比，$Decoder_A$ 和 $Decoder_P$ 採用 $2D-CB-Sigmoid$。最終，CSU 的輸出縮放特徵 $F_A^-$ 和 $F_P^-$ 可按以下方式計算：

$$\begin{cases}\mathbf{F}_\mathrm{A}^-=\mathbf{F}_\mathrm{A}\odot h_\mathrm{A},\\\mathbf{F}_\mathrm{P}^-=\mathbf{F}_\mathrm{P}\odot h_\mathrm{P}.\end{cases} \tag{10}$$\

可以看出，縮放後的特徵 $F_A^-$ 和 $F_P^-$ 保持特徵維度不變，便於嵌入到其他雙流網絡中。

### **C. 跨空間融合單元 (Cross-Spatial Fusion Unit, CFU)**

與時間注意力相反，跨空間融合單元 (CFU) 主要關注學習外觀序列和人體姿勢中的動態空間互補特徵。如圖 6 所示，CFU 的結構與門控融合單元類似，不同之處在於 CFU 的上游輸入被替換為縮放特徵 $F_A^-$ 和 $F_P^-$。此外，為了分離由多通道模態引起的特徵噪聲，CFU 對縮放特徵的通道維度進行了平均池化。

實際上，經過通道池化後的特徵 $\mathbf{F}_\mathrm{A}^s \in \mathbb{R}^{T \times H_P^f \times W_P^f}$ 和 $\mathbf{F}_\mathrm{P}^s \in \mathbb{R}^{T \times H_P^f \times W_P^f}$ 作為 CFU 的輸入。在這種情況下，跨空間對齊特徵和空間門控權重可以透過 CAB* 和 CGB* 來提取，它們分別以 2D-C B-ReLu 和 2D-​​C B-Sigmoid 建構。CFU 更輕量且比使用 3D-CNN 模塊的 Equation (7) 更具通用性。具體來說，CFU 的融合步驟可以參照 Equation (7) 和 (8)，這裡不再重複描述。

## **V. 動作分類**

如圖 2 所示，最終兩個流的特徵會被串聯並由全連接層聚合，隨後通過 softmax 層來計算每個類別的動作得分。我們將該方法的學習過程分為兩個階段：網絡預訓練和融合微調。在第一階段，我們分別訓練雙流編碼網絡 AppearanceNet 和 PoseNet，並通過最小化標準交叉熵損失函數來進行訓練：

$$L_{cls}=-\frac1{\mathcal{N}} \sum_{n=1}^{\mathcal{N}}\sum_{c=1}^{\mathcal{C}}\rho_{c,n}log\left(\hat{\rho}_{c,n}\right), \tag{11}$$\

其中 $\rho_{c,n}$ 和 $\hat{\rho}_{c,n}$ 分別代表真實值和預測得分，$\mathcal{N}$ 是樣本數量。預訓練的 AppearanceNet 和 PoseNet 用於初始化 B2C-AFM 的對應網絡，融合模塊除外，它們是隨機初始化的。在第二階段，融合微調期間，AppearanceNet 和 PoseNet 被固定，我們通過使用微調損失函數訓練 B2C-AFM：

$$
L_{fin} = L_{cls} + \lambda L_{gate}, \tag{12}
$$

其中，Equation (12) 右側的第二項表示門控正則化項，$\lambda$ 是平衡因子。正則化的主要思想是對 CSU 的姿勢門控 $g_P$ 施加約束，遵循的假設是外觀特徵和姿勢特徵應該互補。姿勢門控通過微調二元交叉熵損失函數進行訓練：

$$L_{gate}=-\frac1{\mathcal{N}}\sum_{n=1}^{\mathcal{N}}g_{P,n}log\left(\hat{g}_{P,n}\right)+\left(1-g_{P,n}\right)log\left(1-\hat{g}_{P,n}\right), \tag{13}$$

其中 $g_{P,n}$ 和 $\hat{g}_{P,n}$ 分別表示期望和預測的門控權重。為了響應我們對空間互補性的動機，設置 $g_{P,n}$ 的期望值為 $\frac{1}{2}$。因此，Equation (13) 可以簡化為：

$$L_{gate}=-\frac1{2\mathcal{N}}\sum_{n=1}^{\mathcal{N}}log\left[\hat{g}_{\mathcal{P},n}\left(1-\hat{g}_{\mathcal{P},n}\right)\right]. \tag{14}$$


## **VI. 實現細節**

![[PaperWithCode/14-B2C/Table1.png]]
表 I 基於Resnet-18所建構的B2C-AFM架構

![[PaperWithCode/14-B2C/Table2.png]]
表 II 基於Resnet-50所建構的B2C-AFM架構
### **A. 網絡架構**  
在本文中，我們主要參考 TSM [7]、IntegralAction [6] 和 ResNet [1] 設計了 AppearanceNet 和 PoseNet 的骨幹網絡。我們使用了 ResNet-18 和 ResNet-50 來構建我們方法的網絡，對應的詳細架構如表 I 和表 II 所示。在這些表格中，2D 和 3D 卷積核的維度和步幅分別用 $(T × H × W, C, stride T × H × W)$ 和 $(H × W, C, stride H × W)$ 表示。外觀和姿勢特徵的輸出大小記為 $(∗ : T × H × W) | ∗ ∈ {A, P}$，關於網絡的更多詳細說明可以參考 IntegralAction [6] 和 TSM [7]。我們將關節點熱圖、肢體熱圖和肢體流場的組合作為姿勢序列的輸入，並將其堆疊在一起。此外，ResNet-18 和 ResNet-50 分別用於進行消融實驗和與最先進方法的比較。

### **B. 訓練細節**  
我們的代碼基於 Pytorch 實現，使用 Nvidia GeForce GTX 3090 GPU 進行訓練和測試。在本文的其餘部分，所有模型均以端到端的方式訓練，並且我們採用了隨機梯度下降 (SGD) 優化器，mini-batch 大小設置為 3 以更新梯度下降的權重。學習率設置和權重初始化策略參考了 IntegralAction [6]。RGB 序列和姿勢序列的空間尺寸分別設置為 224 × 224 和 56 × 112。學習階段中使用了數據增強策略 [7]，包括隨機縮放、平移和水平翻轉。最後，為了與最先進方法進行比較，我們將外觀和姿勢序列都設置為 $T = 16$ 幀，並設置幀間隔為 $\tau = 4$。為了進行公平比較，我們遵循 TSM [7] 和 IntegralAction [6] 的做法，對每個視頻的 10 個片段的預測動作得分取平均值作為測試結果。

## **VII. 實驗**

### **A. 數據集和評估指標**

1) **NTU-RGBD**：NTU-RGBD [13] 包含 60 種動作類別和 56880 個視頻樣本。RGB 視頻的原始分辨率為 1920 × 1080，對應的 2D 人體姿勢由 25 個關節點組成。在我們的方法中，僅選取 15 個關節點，包括腳踝、膝蓋、髖部、手腕、肘部、肩部、頭部、胸腔和骨盆。NTU-RGBD 中的人體動作主要涉及三大類別：日常動作、互動動作和醫療條件，動作範圍從上下文無關的動作到上下文相關的動作。我們採用標準的交叉主體拆分 [13] 進行訓練和測試，以測量預測準確性。

2) **Kinetics**：Kinetics [12] 包含 24 萬個訓練視頻和 2 萬個驗證視頻，涵蓋 400 種人體動作類別。該數據集是著名的大規模上下文相關人體動作數據集。我們僅使用其子集 Kinetics50，該子集包括 33K 個訓練視頻和 2K 個驗證視頻，涵蓋 50 種動作類別，參照 Mimetics [48]。在 Kinetics 上的姿勢估計我們使用 ST-GCN [16] 的結果。本文中，Kinetics 的 2D 人體姿勢包含 18 個關節點，我們將其采樣為 16 個關節點，排除了耳朵和眼睛的關節點。

3) **Mimetics**：Mimetics [48] 只包含 713 個視頻，其動作類別與 Kinetics50 的動作類別相同。雖然 Mimetics 和 Kinetics50 擁有相同的動作類別，但前者中的人體動作在不同的場景中進行，主要涉及上下文無關的動作視頻。我們使用與 Kinetics50 中相同的關節點來構建人體姿勢，這些 2D 坐標依賴於 IntegralAction [6] 提供的數據集。由於 Mimetics 的訓練樣本較少，因此該數據集僅用於評估在 Kinetics50 上訓練的模型對未見動作的推斷能力。

根據最新的研究 [6]、[7]、[25]，NTU-RGBD、Kinetics 和 Mimetics 的評估指標選擇了 Top-1 和 Top-5 準確率。預測準確性是對所有人體動作類別的平均值。此外，我們還展示了混淆矩陣，以提供跨類別區分的更多細節。

### **B. 消融實驗**

為了驗證每個貢獻，我們進行了消融實驗，來分析我們對姿勢表示、融合模塊和損失正則化的提議。在這部分中，B2C-AFM（ResNet-18）的骨幹網絡可以參考表 I 中的 AppearanceNet 和 PoseNet。默認情況下，外觀和姿勢序列的幀數 $T = 8$，間隔 $τ = 8$。

![[PaperWithCode/14-B2C/Table3.png]]
表 III NTU-RGBD 上姿勢表示的消融實驗

1) **姿勢表示**：僅使用 PoseNet 來預測動作類別，並在 NTU-RGBD 上呈現消融結果，如表 III 所示。輸入包括關鍵點熱圖、肢體熱圖、PAF [49] 和 Limb Flow Fields。方法 ResNet-18(C) 探索了這些輸入的不同組合，其中 C 是由 J 和 K 計算得出的通道維度。在增加了 0.02M 參數的情況下，Limb Flow Fields 平均提高了約 3.14% 的準確率，驗證了我們創新成果的有效性。與 PAF 控制組相比，Limb Flow Fields 在幾乎相同的模型參數下實現了穩定的整體改進。在某種程度上，運動導向的表示比人體姿勢的空間親和性扮演了更重要的角色。實驗結果顯示，關鍵點熱圖、肢體熱圖和 Limb Flow Fields 的組合達到了最佳性能。
此外，我們還調查了時間模糊性如何影響推理模型的魯棒性。如圖 7 所示，我們列舉了四種類型的姿勢序列來評估不同幀數下的 Top-1 和 Top-5 準確率。我們設置了 $T ∈ {4, 8, 16, 32, 64}$，較小的 $T$ 導致更具挑戰性的時間模糊性。從綠色準確率曲線可以看出，在採樣數量較少的情況下，Limb Flow Fields 顯著提高了模型性能並緩解了時間模糊性。隨著 $T$ 的增加，準確率呈現出先上升後穩定的趨勢，最佳幀數保持在 $T = 16$ 左右。

![[PaperWithCode/14-B2C/Figure7.png]]
圖7.稀疏採樣的穩健性分析。 (a) Top-1 預測精度，(b) Top-5 預測精度。

2) **融合模塊**：如表 IV 所示，我們列舉了不同的融合模塊來結合外觀和姿勢特徵。“RGB_only” 和 “Pose_only” 模型分別依賴於 AppearanceNet 和 PoseNet。與 “RGB_only” 相比，雙向融合單元 (BFU) 提高了 Top-1 準確率 0.9%，代價是模型參數增加了一倍。因此，網絡容量不是性能改進的決定性因素。對於雙流融合模型，有效的融合策略是提高模型性能的基礎。

![[PaperWithCode/14-B2C/Table4.png]]
表 IV 融合模組在 Kinetics50 和 Mimetics 上的燒蝕實驗

整體而言，人體姿勢有助於提高模型在識別 Mimetics 中未見動作時的泛化能力。一方面，門控融合單元 (GFU) 在 Mimetics 上實現了 15.77% 的 Top-1 準確率，證明了空間互補性對上下文動作識別的重要性。另一方面，共同時間尺度單元 (CSU) 將 Kinetics50 的 Top-5 準確率提高到 78.32%，對應於時間一致性的假設。門控和尺度注意機制均被證明有效。

我們設計了融合模塊 “CSU+CFU”，將共同時間尺度單元和跨空間融合單元 (CFU) 結合起來。融合模塊的詳細網絡可以參考表 I。從表 IV 中粗體顯示的最後一行，我們的融合模塊在綜合排名中名列前茅。結合我們的每一項貢獻，“CSU+CFU” 在識別已見和未見動作時表現出平衡的預測能力。

3) **損失正則化**：此外，我們還旨在通過強化損失正則化並探索不同的平衡因子 λ 來調查 B2C-AFM（ResNet-18）的魯棒性，λ 控制對姿勢門的約束強度。如表 V 所示，我們列出了一個 λ ∈ {0.0, 0.5, 1.5, 4.5} 的列表，λ 值越大，對模型的約束越強。每個模型訓練了五次，計算其預測準確率的均值和標準差，如 77.65% ± 0.38。

儘管引入損失正則化後，B2C-AFM（ResNet-18）的性能提升不大，但大多數消融模型的標準差趨於降低。確實，較大的 λ 會正則化這些模型，從外觀和姿勢流中選擇平衡特徵。對於 Mimetics，降低的標準差證明了多種同質模態可以提高在已見和未見動作中的魯棒性能。此外，我們發現 λ = 4.5 在性能波動較小的情況下實現了更高的準確率，我們在後續實驗中採用了這一設置。

![[PaperWithCode/14-B2C/Table5.png]]
表 V 在 Kinetics50 和模擬物上使用不同 λ 進行訓練的 B2C-AFM (Resnet-18) 的 TOP-1 和 TOP-5 準確度

### **C. 與最先進方法的比較**

為了進行令人信服的比較，我們報告了 B2C-AFM（ResNet-18）和 B2C-AFM（ResNet-50）在三個公開數據集上的準確率。它們的網絡結構參考表 I 和表 II，實現細節參考第 V 部分。結果與大多數具有代表性的最先進方法進行比較，包括 IntegralAction [6]、STAR-Transformer [54]、SlowFast [2] 等。正如表 VI 所示，我們的方法雖不如 STAR-Transformer [54]，但在 NTU-RGBD 上與最新的基於 GCN 的模型具有競爭力的準確率。ResNet-18 和 ResNet-50 的積極結果顯示，我們的融合模塊可以快速遷移到不同的網絡骨幹結構。參考表 VII，我們在 Kinetics 50 和 Mimetics 上實現了最先進的性能。我們將 Kinetics50 的 Top-1 和 Top-5 準確率分別提高到 82.9% 和 96.9%，大幅超過了 IntegralAction [6]。在 Mimetics 的改進中，我們的方法在 Kinetics50 上訓練後可以更好地泛化到未見動作。然而，在 Mimetics 中的有限性能表明，監督學習與非監督動作識別之間仍然存在較大的性能差距。

![[PaperWithCode/14-B2C/Table6.png]]
表 VI NTU-RGBD 與 SOTA 方法的精確度比較

![[PaperWithCode/14-B2C/Table7.png]]
表 VII KINETICS50 和 MIMETICS 與 SOTA 方法的準確性比較
### **D. 效率和實際應用**

我們在 Nvidia 3090 GPU 上測試了我們的模型。使用 ResNet-18 作為骨幹網絡，RGB_only 和 Pose_only 模型分別平均需要 51.7 毫秒來推斷一段人體動作，融合模型則需要約 61.2 毫秒。我們的融合模塊引入了約 18.4% 的額外計算時間，這是可以接受的，因為其接近實時運行的效率為 16.3 FPS。此外，基於 ResNet-50 構建的單流和雙流模型平均需要 101.3 毫秒和 145.7 毫秒。

在實際應用中，我們可以利用滑動窗口來處理視頻流。默認情況下，每個窗口緩存約 64 幀圖像，並從中隨機抽樣 8 張圖像用於人體識別。對於人體姿勢，我們可以使用 FasterRCNN [56] 進行人體檢測，並使用 HRNet [57] 進行人體姿勢估計。這種自上而下的策略可以自動檢測和分配 2D 人體骨架給視頻中的所有人物。根據本文中的姿勢表示，我們可以將這些骨架預處理為圖 2 中顯示的姿勢序列。最後，我們可以將外觀和姿勢序列輸入模型以推斷人體動作的類別。

### **E. 定性結果與討論**

圖 8 和圖 9 中的混淆矩陣補充了動作區分的更多細節。一方面，B2C-AFM（ResNet-50）能夠準確識別 NTU-RGBD 中的以身體為中心的動作，例如“戴帽子/帽子”、“跳起”和“踉蹌”。另一方面，B2C-AFM（ResNet-50）可以識別 Kinetics 中的上下文動作，例如“衝浪”、“擊打棒球”和“系領帶”。如圖 10 和圖 11 所示，這些動作同時包含人體和物體。在圖 11（a）中，由於遮擋和照明條件不佳導致的人體姿勢不佳，我們的方法在上下文交互和上下文外動作中產生了穩健的性能。

然而，在推斷 Mimetics 中的未見動作時，仍然存在很多挑戰，例如“寫作”和“讀書”。比較圖 11（a）和（b），不同的文化背景、社會領域和動作習慣可能導致不同的人體動作。因此，通過常見文本感知 [58]、場景圖 [59] 和運動原語 [60] 表示的異質模式可能是實現通用動作識別的關鍵。我們將這一挑戰留待未來的工作解決。

![[PaperWithCode/14-B2C/Figure8.png]]
圖 8. B2C-AFM (ResNet-50) 在 NTU-RGBD 上的混淆矩陣。

![[PaperWithCode/14-B2C/Figure9.png]]
圖 9. B2C-AFM (ResNet-50) 在 Kinetics50 和 Mimetics 上的混淆矩陣。 (a) 動力學50 (b) 模擬。

![[PaperWithCode/14-B2C/Figure10.png]]
圖 10. NTU-RGBD 的定性結果。

![[PaperWithCode/14-B2C/Figure11.png]]
圖 11. Kinetics50 和 Mimetics 的定性結果。 (a) Kinetics50 和 (b) Mimetics 中的「吸煙」、「寫作」和「閱讀」動作。

## **VIII. 結論**

本文提出了一種新穎的雙向共同時間和跨空間注意力融合模型，用於人體動作識別。受到同質模態會保持時間一致性和空間互補性的觀察啟發，B2C-AFM 依次引入了 CSU 和 CFU 來整合外觀和姿勢特徵。一方面，CSU 探索並利用了相同時間節奏下的動作約束，通過共享的共同時間注意編碼器來實現。另一方面，CFU 遵循空間門控機制，從場景外觀和人體姿勢中選擇互補特徵。這種異步融合策略可以完全適應空間和時間之間的異向性。為了解決稀疏採樣引起的時間模糊性，我們探索了 Limb Flow Fields，這是一種新穎的運動導向姿勢表示，將肢體運動簡化為基本的平移和旋轉。人體姿勢的顯式表示可以指示人體運動的方向和順序。

在三個具有挑戰性的公共數據集上進行的實驗驗證了每個貢獻的有效性，其在上下文和上下文外動作識別中的性能具有競爭力。尤其是我們的方法在分類未見動作時顯著超越了其他具有代表性的方法。同時，充分的消融實驗表明我們在已見和未見人體動作中的魯棒性。然而，推斷未見動作時的有限準確率強調了常見文本感知、場景圖和運動原語的重要性。我們將這些監督學習和非監督動作識別之間的挑戰留待未來的工作解決。