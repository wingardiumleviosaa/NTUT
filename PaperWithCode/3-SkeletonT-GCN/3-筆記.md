

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22HAUZUV4T%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B390.2%2C241.662%2C504.003%2C250.3%5D%2C%5B108%2C230.753%2C504.001%2C239.391%5D%2C%5B108%2C219.844%2C429.054%2C228.482%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=2&#x26;annotation=HAUZUV4T">we implement a lightweight Instance Pooling module before the GCN models. The key idea is to aggregate the features of multiple persons and projects them to a single skeletal representation in the early stage.</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 2</a></span>)</span>\
可以在不增加計算成本的情況下支持群體活動的分類

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22BDC7EFE3%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B475.121%2C603.449%2C505.241%2C612.087%5D%2C%5B108%2C592.54%2C503.997%2C601.178%5D%2C%5B108%2C581.631%2C215.665%2C590.269%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=3&#x26;annotation=BDC7EFE3">However, point cloud data introduces too much redundant information for learning action patterns, leading to high computation costs.</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 3</a></span>)</span>

*   點雲方法的優勢：基於點雲的動作識別方法相比於基於 RGB 的方法更具魯棒性，特別是在應對光照變化和觀察角度變化時表現得更穩定。這是因為點雲數據能提供更準確的三維位置資訊。

*   點雲的挑戰：雖然一些研究使用無序的 3D 點雲數據作為輸入，但點雲數據往往包含過多冗餘信息，這會增加計算成本並影響模型的效率。因此，使用點雲數據來學習動作模式的代價相對較高。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22ZS5WVBII%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B388.589%2C384.311%2C504.001%2C392.949%5D%2C%5B108%2C373.402%2C504.002%2C382.04%5D%2C%5B108%2C362.493%2C503.997%2C371.131%5D%2C%5B108%2C351.584%2C504.001%2C360.222%5D%2C%5B108%2C340.675%2C504.353%2C349.313%5D%2C%5B108%2C329.766%2C503.997%2C338.404%5D%2C%5B108%2C318.856%2C366.201%2C327.494%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=3&#x26;annotation=ZS5WVBII">However, traditional methods commonly face two limitations: (1) they maintain a static skeleton structure with a fixed number of keypoints, which restricts their ability to capture multi-scale information, and (2) the computational costs linearly increase with each additional person, resulting in the input being cropped to a maximum of two individuals. In this work, we propose a Skeleton Transformation strategy to dynamically modify the skeleton structure and downsample keypoints. Additionally, we introduce an Instance Pooling module to overcome the constraints of input individuals.</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 3</a></span>)</span>\
STGCN 開創了將 GCN 應用於骨架動作識別的先河。傳統基於 GCN 的方法面臨兩個主要限制：\
1\. 它們維持一個靜態的骨架結構，關鍵點數量固定，這限制了模型捕捉多尺度信息的能力，無法應對不同粒度的動作變化。\
2\. 計算成本隨著每新增一個人物而線性增加，導致輸入只能裁剪到最多包含兩個人，這限制了處理多個人物的能力。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22HWPBU4NM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B310.647652584%2C91.6660784%2C503.9972105879994%2C100.30365259999999%5D%2C%5B108%2C80.75707840000001%2C504.00079700399976%2C89.3946526%5D%2C%5B108%2C69.8480784%2C466.44438539999965%2C78.4856526%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=3&#x26;annotation=HWPBU4NM">firstly extract human bounding boxes using the ResNet50-based Faster-RCNN [21]. Subsequently, the COCO-WholeBody [25] keypoints within specified bounding boxes are obtained through the pre-trained human pose estimator [48].</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 3</a></span>)</span>\
提取過程首先使用基於 ResNet50 的 Faster-RCNN 來檢測人體的邊界框，接著通過預訓練的人體姿態估計器(高解析度表示學習方法)來獲得邊界框內的 COCO-WholeBody 關鍵點。
高解析度表示學習方法 [deep-high-resolution-net](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch):  https://arxiv.org/abs/1902.09212

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22859G8H2E%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B306.725%2C321.564%2C505.745%2C330.202%5D%2C%5B107.532%2C310.655%2C504.004%2C319.293%5D%2C%5B108%2C299.746%2C503.997%2C308.384%5D%2C%5B108%2C288.837%2C430.4%2C297.475%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%224%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=4&#x26;annotation=859G8H2E">More details and results are provided in Sec. D. We find facial keypoints (23-90th) have higher video variance and lower motion frequency, which indicates low contribution for action recognition. This observation guides us to manually remove them, resulting in the formation of the final Expressive Keypoints representation.</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 4</a></span>)</span>\
影片方差計算每個人關鍵點的變化，運動方差則計算每個關鍵點在幀之間的運動頻率。這兩個指標幫助分析哪些關鍵點對動作識別的貢獻較低。最終臉部的關鍵點（23-90號關鍵點）具有較高的影片方差和較低的運動頻率，被手動移除，最終形成了精簡且表達力強的關鍵點表示，稱為 Expressive Keypoints representation。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22KBW7GA6R%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B107.691%2C146.212%2C504.005%2C154.85%5D%2C%5B108%2C135.303%2C504.35%2C143.941%5D%2C%5B108%2C124.054%2C505.385%2C134.017%5D%2C%5B108%2C113.484%2C503.997%2C122.122%5D%2C%5B108%2C102.575%2C504.003%2C111.213%5D%2C%5B108%2C91.666%2C503.997%2C100.304%5D%2C%5B108%2C80.757%2C503.997%2C89.395%5D%2C%5B108%2C69.848%2C301.887%2C78.486%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%224%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=4&#x26;annotation=KBW7GA6R">To this end, we propose a novel Skeleton Transformation (SkeleT) strategy to gradually downsamples the Expressive Keypoints throughout the processing stages. The SkeleT strategy can be seamlessly integrated into most GCN methods to create our SkeleT-GCN (e.g baseline: DGSTGCN [13] → ours: SkeleT-DGSTGCN) without modifying the inner implementation of their graph convolution and temporal convolution layers or the high-level architectural design. What we do is to encapsulating the baseline graph convolution layers within a proposed Grouped Mapping framework, where the input keypoint features are divided into groups and multiplied with the mapping matrices before being processed by the graph convolution layers.</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 4</a></span>)</span>\
骨架轉換策略（SkeleT），該策略通過處理階段逐步對 Expressive Keypoints 進行下採樣，動態地調整骨架結構。方法將基線 GCN 模型中的圖卷積層包裹在一個名為「分組映射框架（Grouped Mapping framework）」的框架內。在這個框架中，輸入的關鍵點特徵被劃分成不同的組，並在進入圖卷積層之前與映射矩陣相乘。\
\
通過有效利用 Expressive Keypoints，SkeleT-GCN 可以在計算量（GFLOPs）大幅降低的情況下，實現與基線 GCN 方法相當甚至更高的精度。例如，將基線的 DGSTGCN \[13] 改進為 SkeleT-DGSTGCN，大幅提升了效率而不需要大改模型結構。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22572XDBM6%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B137.888%2C553.58%2C284.936%2C562.536%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%225%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=5&#x26;annotation=572XDBM6">Preliminary and notations of GCN</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 5</a></span>)</span>\
基於圖卷積網路（GCN）的骨架序列模型的架構。

1. 骨架序列 X 的定義：
   - 骨架序列 \\( X \in \mathbb{R}^{J \times T \times C} \\) 表示有 \\( J \\) 個關節點（joints），每個關節點有 \\( C \\) 個維度通道（例如：x, y, z 空間坐標），並且序列長度為 \\( T \\) 幀。也就是說，這是一個三維張量，代表了在一段時間內每個關節的空間位置。\

2. 空間-時間區塊架構

在大多數現有的基於 GCN 的方法中，模型通常由 M 個**空間-時間區塊**（spatial-temporal blocks）組成。每個空間-時間區塊 F 包含兩個主要部分：

- 圖卷積層 G：用來建模骨架關節之間的空間關係。

- 時間卷積層 T：用來建模不同時間幀上的變化，捕捉時間信息。這些區塊交替處理空間和時間信息。

3. 區塊索引集合 B：
B={1,2,…,M} 表示所有空間-時間區塊的索引集。分成兩個子集：
     - B\_d 包含了下採樣（downsampling）區塊的索引，這些區塊會縮短時間序列的長度。
     - B\_n 則包含其他正常的區塊索引，不會對時間序列進行下採樣。
這樣的區塊劃分有助於模型在處理長時間序列時，通過下採樣區塊 F^d 有效縮短時間長度，同時在正常區塊 F^n 保持特徵提取的細緻度。

4. 鄰接矩陣 A：
   - \$A\in \mathbb{R}^{J \times J}\$ 是描述骨架關節之間連接關係的鄰接矩陣。如果第  i  個關節和第  j  個關節是物理相連的，則 \$A\_{ij} = 1\$，否則為 0。

5. 區塊 F 的計算公式：
     其中，\$\tilde{A} = A + I\$ 表示加上自連接的骨架拓撲圖（即在每個關節上加一個自連接邊），這樣模型不僅能考慮關節之間的關係，還能保留每個關節本身的特徵。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22IDYQMDBU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B137.888%2C386.199%2C269.892%2C395.155%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%225%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=5&#x26;annotation=IDYQMDBU">Grouped Mapping Framework</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 5</a></span>)</span>\
- 目標：為了實現 SkeleT 策略，Grouped Mapping Framework 封裝了原有的圖卷積層 GGG 和時間卷積層 T，而不需要修改它們的內部設計。該框架將模型的輸入通道分組進行處理。
- 架構：骨架序列 X 的通道維度被分為 K 個組，每組的特徵通道寬度為 C/K。每一組的特徵都會獨立與對應的映射矩陣 M 相乘，從而動態調整骨架結構。
- 特徵提取與融合：之後，K 個圖卷積層並行運行來提取不同組的特徵，最終再將這些組特徵沿著通道維度進行拼接，並由時間卷積層處理以捕捉時間依賴，生成精煉的運動特徵。

> <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FXUEUBMYZ%22%2C%22annotationKey%22%3A%22FWANK9BY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B108%2C302.609%2C234.834%2C311.565%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/XUEUBMYZ?page=6&#x26;annotation=FWANK9BY">3.3 Instance Pooling module</a></span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F15044111%2Fitems%2FHRQRJVUX%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/HRQRJVUX">Yang …等, 2024, p. 6</a></span>)</span>
**Instance Pooling (IP) 模塊** 是一種用於多人體活動識別的輕量化擴展模塊。在將多個輸入骨架序列傳遞給 GCN 之前，提前進行特徵融合。這樣的早期融合可以減少後續的計算量，因為無論有多少個輸入人物，空間-時間建模只需要執行一次。
- 處理流程：使用全連接層獲取每個關鍵點的嵌入表示，並對其進行位置編碼。接著，應用由 \[22] 提出的 拼接池化層（Concat Pool Layer）和 分組池化層（Group Pool Layer）來聚合 I 個實例特徵向量。
